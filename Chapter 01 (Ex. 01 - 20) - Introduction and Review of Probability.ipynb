{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1**.  Let $A_1$ and $A_2$ be arbitrary events and show that $\\text{Pr}\\{A_1 \\cup A_2\\} + \\text{Pr}\\{A_1 A_2\\} = \\text{Pr}\\{A_1\\} + \\text{Pr}\\{A_2\\}$.  Explain which parts of the sample space are being double counted on both sides of this equation and which parts are being counted once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  Consider the disjoint events $A_1 - A_2, A_2 - A_1, A_1 A_2$, where $X - Y = \\{ \\omega : \\omega \\in X \\text{ and not } \\omega \\in Y \\}$.  \n",
    "\n",
    "By construction these events are disjoint: any event $\\omega$ either:\n",
    "\n",
    "- belongs to $A_1$ but not $A_2$, in which case $\\omega \\in A_1 - A_2$ but it does not belong to $A_2 - A_1$ or $A_1 A_2$;\n",
    "- belongs to $A_2$ but not $A_1$, in which case $\\omega \\in A_2 - A_1$ but it does not belong to $A_1 - A_2$ or $A_1 A_2$;\n",
    "- belongs to both $A_1$ and $A_2$, in which case $\\omega \\in A_1 A_2$ but it does not belong to $A_1 - A_2$ or $A_2 - A_1$;\n",
    "- does not belong to $A_1$ or $A_2$, in which case it does not belong to any of $A_1 - A_2, A_2 - A_1, A_1 A_2$.\n",
    "\n",
    "Also note that:\n",
    "\n",
    "- $(A_1 - A_2) \\cup A_1 A_2 = A_1$, since any event that belongs to $A_1$ either belongs to $A_2$ (in which case it belongs to $A_1 A_2$) or not (in which case it belongs to $A_1 - A_2$).\n",
    "- $(A_2 - A_1) \\cup A_1 A_2 = A_2$, by a similar argument.\n",
    "- $(A_1 - A_2) \\cup (A_2 - A_1) \\cup A_1 A_2 = A_1 \\cup A_2$, by applying the union over the previous two statements.\n",
    "\n",
    "By the third axiom of probability (using the finite version -- add infinitely many empty to get an infinite sequence) we get:\n",
    "\n",
    "1.  $\\text{Pr}\\{A_1 - A_2\\} + \\text{Pr}\\{ A_1 A_2 \\} = \\text{Pr}\\{A_1\\} $\n",
    "2.  $\\text{Pr}\\{A_2 - A_1\\} + \\text{Pr}\\{ A_1 A_2 \\} = \\text{Pr}\\{A_2\\} $\n",
    "3.  $\\text{Pr}\\{A_1 \\cup A_2 \\} = \\text{Pr}\\{A_1 - A_2\\} + \\text{Pr}\\{ A_2 - A_1 \\} + \\text{Pr}\\{ A_1 A_2 \\}$\n",
    "\n",
    "Adding these three statements, we get the desired result,\n",
    "\n",
    "$$\\text{Pr}\\{A_1 \\cup A_2\\} + \\text{Pr}\\{A_1 A_2\\} = \\text{Pr}\\{A_1\\} + \\text{Pr}\\{A_2\\}$$\n",
    "\n",
    "Note that:\n",
    "\n",
    "- If $\\omega \\in A_1$ and $\\omega \\in A_2$, it is counted twice in each side of the equation -- once in $\\text{Pr}\\{A_1 \\cup A_2\\}$, once in $\\text{Pr}\\{A_1 A_2\\}$, once in $\\text{Pr}\\{A_1\\}$ and once in $\\text{Pr}\\{A_2\\}$.\n",
    "- If $\\omega \\in A_1$ but $\\text{not } \\omega \\in A_2$, it is counted once in each side of the equation -- once in $\\text{Pr}\\{A_1 \\cup A_2\\}$ and once in $\\text{Pr}\\{A_1\\}$.\n",
    "- If $\\omega \\in A_2$ but $\\text{not } \\omega \\in A_1$, it is counted once in each side of the equation -- once in $\\text{Pr}\\{A_1 \\cup A_2\\}$ and once in $\\text{Pr}\\{A_2\\}$.\n",
    "- If $\\text{not } \\omega \\in A_1$ and $\\text{not } \\omega \\in A_2$, it is counted zero times in each side of the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2**.  This exercise derives the probability of an arbitrary (non-disjoint) union of events, derives the union bound, and derives some useful limit expressions.\n",
    "\n",
    "**(a)** For two arbitrary events $A_1$ and $A_2$, show that\n",
    "\n",
    "$$ A_1 \\cup A_2 = A_1 \\cup (A_2 - A_1) \\quad \\text{where } A_2 - A_1 = A_2 A_1^c$$\n",
    "\n",
    "Show that $A_1$ and $A_2 - A_1$ are disjoint.  Hint:  Venn diagrams were invented to help understand expressions like these.\n",
    "\n",
    "**(b)** For an arbitrary sequence of events, $\\{A_n : n \\geq 1\\}$, let $B_1 = A_1$ and for each $n \\geq 2$ define $B_n = A_n - \\cup_{m=1}^{n - 1} A_m$.  Show that $B_1, B_2, \\dots$ are disjoint events and show that for each $n \\geq 2$, $\\cup_{m=1}^n A_m = \\cup_{m=1}^n B_m$.  Hint: Use induction.\n",
    "\n",
    "**(c)** Show that\n",
    "\n",
    "$$ \\text{Pr}\\left\\{ \\cup_{n=1}^\\infty A_n \\right\\} = \\text{Pr}\\left\\{ \\cup_{n=1}^\\infty B_n \\right\\} = \\sum_{n=1}^\\infty \\text{Pr} \\left\\{ B_n \\right\\} $$\n",
    "\n",
    "Hint: Use the axioms of probability for the second equality.\n",
    "\n",
    "**(d)** Show that for each $n$, $\\text{Pr}\\left\\{ B_n \\right\\} \\leq \\text{Pr} \\left\\{ A_n \\right\\}$.  Use this to show that\n",
    "\n",
    "$$ \\text{Pr}\\left\\{ \\cup_{n=1}^\\infty A_n \\right\\} \\leq \\sum_{n=1}^\\infty \\text{Pr} \\left\\{ A_n \\right\\} $$\n",
    "\n",
    "**(e)** Show that $\\text{Pr}\\left\\{ \\cup_{n=1}^\\infty A_n \\right\\} = \\lim_{m \\rightarrow \\infty} \\text{Pr} \\left\\{ \\cup_{n=1}^m A_n \\right\\}$.  Hint: Combine (c) and (b).  Note that this says that the probability of a limit of unions is equal to the limit of probabilities.  This might well appear to be obvious without a proof, but you will see situations later where similar appearing interchanges cannot be made.\n",
    "\n",
    "**(f)** Show that $\\text{Pr}\\left\\{ \\cap_{n=1}^\\infty A_n \\right\\} = \\lim_{n \\rightarrow \\infty} \\text{Pr}\\left\\{ \\cap_{i=1}^n A_i \\right\\}$.  Hint: Remember De Morgan's equalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "&\\omega \\in A_1 \\cup A_2 \\\\\n",
    "&\\Longleftrightarrow (\\omega \\in A_1) \\lor (\\omega \\in A_2) \\\\\n",
    "&\\Longleftrightarrow (\\omega \\in A_1) \\lor (\\omega \\in A_2 \\land \\neg(\\omega \\in A_1)) \\\\\n",
    "&\\Longleftrightarrow (\\omega \\in A_1) \\lor (\\omega \\in (A_2 - A_1)) \\\\\n",
    "&\\Longleftrightarrow \\omega \\in A_1 \\cup (A_2 - A_1) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we can explicitly verify the third step with a truth table:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc|c|c}\n",
    "\\omega \\in A_1 & \\omega \\in A_2 & (\\omega \\in A_1) \\lor (\\omega \\in A_2) & (\\omega \\in A_1) \\lor (\\omega \\in A_2 \\land \\neg(\\omega \\in A_1)) \\\\\n",
    "\\hline\n",
    "T & T & T & T \\\\\n",
    "T & F & T & T \\\\\n",
    "F & T & T & T \\\\\n",
    "F & F & F & F\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Since the sets $A_1 \\cup A_2$ and $A_1 \\cup (A_2 - A_1)$ have equivalent statements determining whether a granular event $\\omega$ belongs to them, they are equal,\n",
    "\n",
    "$$ A_1 \\cup A_2 = A_1 \\cup (A_2 - A_1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Let's use induction.\n",
    "\n",
    "For the base case, $n = 2$, we have $B_2 = A_2 - A_1$.  \n",
    "- Note that $B_1 = A_1$ and $B_2$ are disjoint events by construction: every element from $A_1$ is explicitly excluded from $B_2$.  \n",
    "- Also note that $A_1 \\cup A_2 = B_1 \\cup B_2$, since $B_1 \\cup B_2 = A_1 \\cup (A_2 - A_1)$, and this is the result proved in (a).\n",
    "\n",
    "For the induction step, assume the result is true for $n$.  \n",
    "- Since $B_{n + 1} = A_n - \\cup_{i=1}^n A_m$, any element belonging to $B_{n+1}$ will not belong to any of the $A_m$ for $1 \\leq m \\leq n$ by construction, and since from the previous step $\\cup_{m=1}^n A_m = \\cup_{m=1}^n B_m$, it will also not belong to any $B_m$, $1 \\leq m \\leq n$.  Therefore there is no overlap between $B_{n + 1}$ and $B_m$, and the $B_i$'s are disjoint.\n",
    "- Finally, note that, from applying (a) on sets $A_{n + 1}$ and $\\cup_{m=1}^n A_m$,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "A_{n + 1} \\cup \\left( \\cup_{m=1}^n A_m \\right) &= \\left( \\cup_{m=1}^n A_m \\right) \\cup \\left(A_{n+1} - \\left( \\cup_{m=1}^n A_m \\right)  \\right) \\\\ \n",
    "A_{n + 1} \\cup \\left( \\cup_{m=1}^n A_m \\right) &= \\left( \\cup_{m=1}^n B_m \\right) \\cup B_{n+1} \\\\\n",
    "\\cup_{m=1}^{n+1} A_m &= \\cup_{m=1}^{n+1} B_m \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  The result follows from the third axiom of inequality, since all of the $B_i$'s are disjoint by (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  We have $B_n = A_n - \\cup_{m=1}^{n - 1} A_m = A_n - \\cup_{m=1}^{n - 1} B_m$, and so uniting both sides to $\\cup_{m=1}^{n - 1} B_m$ we get $\\cup_{m=1}^{n} B_m = A_n$.  Since the $B_i$'s are disjoint, by the third axiom of probability we have\n",
    "\n",
    "$$ \\text{Pr}\\left\\{A_n\\right\\} = \\sum_{m=1}^n \\text{Pr}\\left\\{ B_m \\right\\} = \\sum_{m=1}^{n - 1} \\text{Pr}\\left\\{ B_m \\right\\} + \\text{Pr}\\left\\{B_n\\right\\}$$\n",
    "\n",
    "and, since for the first axiom of probability,  $\\text{Pr} \\left\\{ B_m \\right\\} \\geq 0$ for $1 \\leq m \\leq n - 1$, we get\n",
    "\n",
    "$$ \\text{Pr}\\left\\{A_n\\right\\} \\geq \\text{Pr}\\left\\{B_n\\right\\} $$\n",
    "\n",
    "Finally, using this inequality for each $n$, and from (b), we have\n",
    "\n",
    "$$ \\text{Pr}\\left\\{ \\cup_{n=1}^\\infty A_n \\right\\} \n",
    "= \\text{Pr}\\left\\{ \\cup_{n=1}^\\infty B_n \\right\\} \n",
    "= \\sum_{n=1}^\\infty \\text{Pr} \\left \\{ B_n \\right\\}\n",
    "\\leq \\sum_{n=1}^\\infty \\text{Pr} \\left\\{ A_n \\right\\} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**  The result from (c) states that\n",
    "\n",
    "$$ \\text{Pr}\\left\\{ \\cup_{n=1}^\\infty A_n \\right\\} = \\text{Pr}\\left\\{ \\cup_{n=1}^\\infty B_n \\right\\} = \\sum_{n=1}^\\infty \\text{Pr} \\left\\{ B_n \\right\\} $$\n",
    "\n",
    "In particular, we can consider a different sequence of sets, $A'_1, A'_2, \\dots$ and $B'_1, B'_2, \\dots$, where\n",
    "\n",
    "$$\n",
    "A'_k = \\begin{cases}\n",
    "A_k &\\text{if } k \\leq n \\\\\n",
    "A_n &\\text{if } k > n\n",
    "\\end{cases}\n",
    "\\quad \\text{and} \\quad\n",
    "B'_k = \\begin{cases}\n",
    "B_k &\\text{if } k \\leq n \\\\\n",
    "\\varnothing &\\text{if } k >n\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Since the previous results also apply to these sequences of sets, the finite version of the result is also valid for every $n$,\n",
    "\n",
    "$$ \\text{Pr}\\left\\{ \\cup_{m=1}^n A_m \\right\\} = \\text{Pr}\\left\\{ \\cup_{m=1}^n B_m \\right\\} = \\sum_{m=1}^n \\text{Pr} \\left\\{ B_m \\right\\} $$\n",
    "\n",
    "But\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\sum_{m=1}^n \\text{Pr} \\left\\{ B_m \\right\\} = \\sum_{n=1}^\\infty \\text{Pr} \\left\\{ B_n \\right\\} $$\n",
    "\n",
    "which exists and is bound between 0 and 1 inclusive, since the right hand value the probability $\\text{Pr}\\left\\{ \\cup_{n=1}^\\infty A_n \\right\\}$.\n",
    "\n",
    "Therefore, by taking the limit to the result on finite sequences, we get\n",
    "\n",
    "$$  \\lim_{n \\rightarrow \\infty} \\text{Pr}\\left\\{ \\cup_{m=1}^n A_m \\right\\} = \\sum_{n=1}^\\infty \\text{Pr} \\left\\{ B_n \\right\\} = \\text{Pr}\\left\\{ \\cup_{n=1}^\\infty A_n \\right\\} $$\n",
    "\n",
    "which is the desired result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)**  De Morgan's equality states that\n",
    "\n",
    "$$ \\left(\\cap_{n=1}^\\infty A_n \\right)^c = \\cup_{n=1}^\\infty A_n^c $$\n",
    "\n",
    "By applying the result of (e) to the sequence of complements $A_1^c, A_2^c, \\dots$ we get:\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr}\\left\\{ \\cup_{m=1}^n A_m^c \\right\\} = \\text{Pr}\\left\\{ \\cup_{n=1}^\\infty A_n^c \\right\\} $$\n",
    "\n",
    "and using the fact that $\\text{Pr}\\left\\{ X \\right\\} + \\text{Pr}\\left\\{ X^c \\right\\} = 1$ we get\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} 1 - \\text{Pr}\\left\\{ \\cap_{m=1}^n A_m \\right\\} = 1 - \\text{Pr}\\left\\{ \\cap_{n=1}^\\infty A_n \\right\\} $$\n",
    "\n",
    "from which the result follows,\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr}\\left\\{ \\cap_{m=1}^n A_m \\right\\} = \\text{Pr}\\left\\{ \\cap_{n=1}^\\infty A_n \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.3**.  Find the probability that a five-card poker hand, chosen randomly from a 52-card deck, contains four aces.  That is, if all $52!$ arrangements of a deck of cards are equally likely, what is the probability that all four aces are in the first five cards of the deck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**. There are $52 \\cdot 51 \\cdot 50 \\cdot 49 \\cdot 48$ ordered sequences for the first five cards of the deck.  Of those, the sequences containing four aces correspond to choosing the four aces in some order ($4 \\cdot 3 \\cdot 2 \\cdot 1$), then selecting a fifth card ($48$ choices) and inserting it somewhere alongside the first four cards ($5$ position choices).\n",
    "\n",
    "Therefore, the desired probability is\n",
    "\n",
    "$$ \\frac{(4 \\cdot 3 \\cdot 2 \\cdot 1) \\cdot 48 \\cdot 5}{52 \\cdot 51 \\cdot 50 \\cdot 49 \\cdot 48} = \\frac{1}{54145} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.4**. Consider a sample space of eight equiprobable sample points and let $A_1, A_2, A_3$ be three events each of probability $1/2$ such that $\\text{Pr}\\{ A_1 A_2 A_3 \\} = \\text{Pr}\\{A_1\\}\\text{Pr}\\{A_2\\}\\text{Pr}\\{A_3\\}$.\n",
    "\n",
    "**(a)** Create an example where $\\text{Pr}\\{ A_1 A_2 \\} = \\text{Pr}\\{ A_1 A_3 \\} = 1/4$ but $\\text{Pr}\\{ A_2 A_3 \\} = 1/8$.  Hint:  make a table with a row for each sample point and a column for each event and try different ways of assigning sample points to events (the answer is not unique).\n",
    "\n",
    "**(b)** Show that, for your example, $A_2$ and $A_3$ are not independent.  Note that the definition of statistical independence would be very strange indeed if it allowed $A_1, A_2, A_3$ to be independent while $A_2$ and $A_3$ are dependent.  This illustrates why the definition of independence requires (1.14) rather than just (1.15)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "Let the sample space be $\\Omega = \\{ \\omega_i : 1 \\leq i \\leq 8, i \\in \\mathbb{N} \\}$, with a probability distribution $\\text{Pr}\\{ \\omega_i \\} = 1/8$ for each $i$.  Consider the events $A_1, A_2, A_3$ specified by:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|ccc|c|c|c|c}\n",
    "\\omega & A_1 & A_2 & A_3 & A_1 A_2 A_3 & A_1 A_2 & A_1 A_3 & A_2 A_3 \\\\\n",
    "\\hline\n",
    "\\omega_1 & T & T & T & T & T & T & T \\\\ \n",
    "\\omega_2 & T & T & F & F & T & F & F \\\\\n",
    "\\omega_3 & T & F & T & F & F & T & F \\\\\n",
    "\\omega_4 & T & F & F & F & F & F & F \\\\\n",
    "\\omega_5 & F & T & F & F & F & F & F \\\\\n",
    "\\omega_6 & F & T & F & F & F & F & F \\\\\n",
    "\\omega_7 & F & F & T & F & F & F & F \\\\\n",
    "\\omega_8 & F & F & T & F & F & F & F \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Or, with set notation:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "A_1 &= \\{ \\omega_1, \\omega_2, \\omega_3, \\omega_4 \\} \\\\\n",
    "A_2 &= \\{ \\omega_1, \\omega_2, \\omega_5, \\omega_6 \\} \\\\\n",
    "A_3 &= \\{ \\omega_1, \\omega_3, \\omega_7, \\omega_8 \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which leads to:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "A_1 A_2 A_3 &= \\{ \\omega_1 \\} \\\\\n",
    "A_1 A_2 &= \\{ \\omega_1, \\omega_2 \\} \\\\\n",
    "A_1 A_3 &= \\{ \\omega_1, \\omega_3 \\} \\\\\n",
    "A_2 A_3 &= \\{ \\omega_1 \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since $\\{ \\omega_i \\}$ are disjoint for each $i$ and $\\text{Pr}\\{ \\omega_i \\} = 1/8$ for each $i$, the desired properties follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  In this example, $A_2$ and $A_3$ are not independent, as $\\text{Pr}\\{A_2\\} = \\text{Pr}\\{ A_3 \\} = 1/2$ and $\\text{Pr}\\{A_2 A_3\\} = 1/8$, so $\\text{Pr}\\{A_2 A_3\\} \\neq \\text{Pr}\\{A_2\\} \\text{Pr}\\{A_3\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.5**.  This exercise shows that for all random variables $X$, $F_X(x)$ is continuous from the right.\n",
    "\n",
    "**(a)** For any given random variable $X$, any real number $x$, and each integer $n \\geq 1$, let $A_n = \\{ \\omega : X > x + 1/  n \\}$, and show that $A_1 \\subseteq A_2 \\subseteq \\cdots$.  Use this and the corollaries to the axioms of probability to show that $\\text{Pr} \\left\\{ \\cup_{n \\geq 1} A_n \\right\\} = \\lim_{n \\rightarrow \\infty} \\text{Pr}\\left\\{A_n\\right\\}$.\n",
    "\n",
    "**(b)** Show that $\\text{Pr}\\left\\{ \\cup_{n \\geq 1} A_n \\right\\} = \\text{Pr}\\left\\{ X > x \\right\\}$ and that $\\text{Pr}\\left\\{ X > x \\right\\} = \\lim_{n \\rightarrow \\infty}\\text{Pr}\\left\\{ X > x + 1 / n \\right\\}$.\n",
    "\n",
    "**(c)** Show that for $\\epsilon > 0$, $\\lim_{\\epsilon \\rightarrow 0} \\text{Pr} \\left\\{ X \\leq x + \\epsilon \\right\\} = \\text{Pr}\\left\\{ X \\leq x \\right\\}$.\n",
    "\n",
    "**(d)** Define $\\tilde{F}_X(x) = \\text{Pr}\\left\\{ X < x \\right\\}$.  Show that $\\tilde{F}_X(x)$ is continuous from the left.  In other words, the continuity from the right for the CDF arises from the almost arbitrary (but universally accepted) choice in defining the CDF as $\\text{Pr}\\left\\{ X \\leq x \\right\\}$ rather than $\\text{Pr}\\left\\{ X < x \\right\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have, for every $n \\geq 1$,\n",
    "\n",
    "$$ \\omega \\in A_n \\Rightarrow X(\\omega) > x + 1/n \\Rightarrow X(\\omega) > x + 1 / (n + 1) \\Rightarrow \\omega \\in A_{n+1}$$\n",
    "\n",
    "and so $A_n \\subseteq A_{n + 1}$, which implies $\\cup_{m=1}^n A_m = A_n$ (since the last set, $A_n$, contains all of the previous ones).\n",
    "\n",
    "Then, from the corollary of the axiom of probability (1.8),\n",
    "\n",
    "$$ \\text{Pr}\\left\\{ \\cup_{n=1}^\\infty A_n \\right\\} = \\lim_{m \\rightarrow \\infty} \\text{Pr} \\left\\{ \\cup_{n=1}^m A_n \\right\\}$$\n",
    "\n",
    "we get the desired result,\n",
    "\n",
    "$$ \\text{Pr}\\left\\{ \\cup_{n \\geq 1} A_n \\right\\} = \\lim_{n \\rightarrow \\infty} \\text{Pr}\\left\\{ A_n \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** We have:\n",
    "\n",
    "$$ \\omega \\in \\cup_{n \\geq 1} A_n \\Rightarrow \\exists n \\geq 1, X(\\omega) > x + 1/n \\Rightarrow X(\\omega) > x \\Rightarrow \\omega \\in \\left\\{ \\omega : X(\\omega) > x \\right\\}$$\n",
    "\n",
    "The implication $\\exists n \\geq 1, X(\\omega) > x + 1/n \\Rightarrow X(\\omega) > x$ can be proven by contradiction:  if $X(\\omega) \\leq x$, then $X(\\omega) \\leq x < x + 1/n$ for all $n$, and so there are no integers $n$ for which $X(\\omega) > x + 1/n$.\n",
    "\n",
    "Given the logic implication above, $\\cup_{n \\geq 1} A_n = \\left\\{ \\omega : X(\\omega) > x \\right\\}$, and so the probability of these two events is the same.  The result now follows from (a):\n",
    "\n",
    "$$ \\text{Pr}\\left\\{ X > x \\right\\} = \\text{Pr} \\left\\{ \\cup_{n \\geq 1} A_n \\right\\} = \\lim_{n \\rightarrow \\infty} \\text{Pr}\\left\\{A_n\\right\\} =  \\lim_{n \\rightarrow \\infty}\\text{Pr}\\left\\{ X > x + 1 / n \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Since the probability of complementary events add up to 1, we can restate the result from (b) as\n",
    "\n",
    "$$ \\text{Pr}\\left\\{ X \\leq x \\right\\} = \\lim_{n \\rightarrow \\infty}\\text{Pr}\\left\\{ X \\leq x + 1 / n \\right\\} $$\n",
    "\n",
    "But this limit is the same as $\\lim_{\\epsilon \\rightarrow 0^+}\\text{Pr}\\left\\{ X \\leq x + \\epsilon \\right\\}$, by replacing each $\\epsilon$ with a potentially smaller value $r(\\epsilon) = 1/n$, where $n = \\lceil 1 / \\epsilon \\rceil$ -- since by construction $\\epsilon \\geq r(\\epsilon)$ and we are just replacing the expression under the limit with an expression that occurs for a later in the limit series:\n",
    "\n",
    "$$\\lim_{\\epsilon \\rightarrow 0^+}\\text{Pr}\\left\\{ X \\leq x + \\epsilon \\right\\} = \n",
    "\\lim_{r(\\epsilon) \\rightarrow 0^+}\\text{Pr}\\left\\{ X \\leq x + r(\\epsilon) \\right\\} = \n",
    "\\lim_{n \\rightarrow \\infty}\\text{Pr}\\left\\{ X \\leq x + 1 / n \\right\\} $$\n",
    "\n",
    "This proves the desired result -- continuity from the right for the CDF $F_X(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  By definition, $\\tilde{F}_X(x) = \\text{Pr}\\left\\{ X < x \\right\\} = 1 - \\text{Pr}\\left\\{ -X \\leq -x \\right\\} = 1 - F_{-X}(-x) $, where $F_{-X}$ is the CDF for the random variable $-X$.  Since the CDF of random variables are continuous from the right as shown in (c), the newly defined function $\\tilde{F}_X(x)$ must be continuous from the left -- since it is a constant minus a right-continuous function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.6**.  Show that for a continuous non-negative random variable $X$,\n",
    "\n",
    "$$ \\int_0^\\infty \\text{Pr}\\left\\{ X > x \\right\\} dx = \\int_0^\\infty x f_X(x) \\; dx \\tag{1.98}$$\n",
    "\n",
    "Hint 1:  First rewrite $\\text{Pr}\\left\\{ X > x \\right\\}$ on the left-hand side of (1.98) as $\\int_x^\\infty f_X(y) dy$.  Then think through, to your level of comfort, how and why the order of integration can be interchanged in the resulting expression.\n",
    "\n",
    "Hint 2:  As an alternative approach, derive (1.98) using integration by parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "$$\n",
    "\\int_0^\\infty \\text{Pr}\\left\\{ X > x \\right\\} dx = \\int_0^\\infty \\int_x^\\infty f_X(y) dy dx\n",
    "= \\int_0^\\infty \\int_0^y f_X(y) dx dy\n",
    "= \\int_0^\\infty y f_X(y) dy\n",
    "$$\n",
    "\n",
    "The order of integration above can be changed since both double integrals sweep the area \n",
    "\n",
    "$$\\{ (x, y) : x \\in [0, \\infty), y \\geq x \\} = \\{ (x, y) : y \\in [0, \\infty), 0 \\leq x \\leq y \\} $$\n",
    "\n",
    "assigning the same integrand to each point, $\\mu(x, y) = f_X(y)$.\n",
    "\n",
    "The result follows by renaming the variable $y$ as $x$ in the last expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.7**.  Suppose $X$ and $Y$ are discrete random variables with the PMF $p_{XY}(x_i, y_j)$.  Show (a picture will help) that this is related to the joint CDF by\n",
    "\n",
    "$$ p_{XY}(x_i, y_j) = \\lim_{\\delta > 0, \\delta \\rightarrow 0} \\left[ F(x_i, y_j) - F(x_i - \\delta, y_j) - F(x_i, y_j - \\delta) + F(x_i - \\delta, y_j - \\delta) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "Since $X, Y$ are discrete random variables, their joint CDF can be expressed as a sum of countably many elements with non-zero probability mass,\n",
    "\n",
    "$$ F(x, y) = \\sum_{a: x_a \\leq x} \\sum_{b : y_b \\leq y} p_{XY}(x_a, y_b) $$\n",
    "\n",
    "Therefore, the expression inside the limit reduces to\n",
    "\n",
    "$$ F(x_i, y_j) - F(x_i - \\delta, y_j) - F(x_i, y_j - \\delta) + F(x_i - \\delta, y_j - \\delta) = \\sum_{a: x_i - \\delta < x_a \\leq x_i} \\sum_{b: y_j - \\delta < y_b \\leq y_j} p_{XY}(x_a, y_b) $$\n",
    "\n",
    "For any $x_a < x_i$ and for any $y_b < y_j$, we can select a $\\delta$ that puts them outside of the interval $(x_i - \\delta, x_i] \\times (y_j - \\delta, y_j]$ -- for example, pick $\\delta = \\min \\{ (x_i - x_a) / 2, (y_j - y_b) / 2 \\}$.  Therefore, all terms other than $p_XY(x_i, y_j)$ will be dropped from the sum on the limit, and the result holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.8**.  A variation of Example 1.5.1 is to let $M$ be a random variable that takes on both positive and negative values with the PMF\n",
    "\n",
    "$$ p_M(m) = \\frac{1}{2 |m| (|m| + 1) }$$\n",
    "\n",
    "In other words, $M$ is symmetric around 0 and $|M|$ has the same PMF as the non-negative random variable of Example 1.5.1.\n",
    "\n",
    "**(a)** Show that $\\sum_{m \\geq 0} m p_M(m) = \\infty$ and $\\sum_{m < 0} m p_M(m) = -\\infty$.  (Thus show that the expectation of $M$ not only does not exist but is undefined even as an extended real number.)\n",
    "\n",
    "**(b)** Suppose that the terms in $\\sum_{m=-\\infty}^\\infty m p_M(m)$ are summed in the order of two positive terms for each negative term (i.e. in the order 1, 2, -1, 3, 4, -2, 5, $\\cdots$). Find the limiting value of the partial sums in this series.  Hint: You may find it helpful to know that\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\left[ \\sum_{i=1}^n \\frac{1}{i} - \\int_1^n \\frac{1}{x} dx \\right] = \\gamma $$\n",
    "\n",
    "where $\\gamma$ is the Euler-Mascheroni constant, $\\gamma = 0.57721\\cdots$\n",
    "\n",
    "**(c)** Repeat (b) where, for any given integer $k > 0$, the order of summation is $k$ positive terms for each negative term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** We have:\n",
    "\n",
    "$$ \\sum_{m=1}^n m p_M(m) = \\frac{1}{2(m + 1)} = \\frac{1}{2} \\sum_{m=2}^n \\frac{1}{m} = \\frac{1}{2} \\left( H_n - 1 \\right) $$\n",
    "\n",
    "where $H_n = \\sum_{m=1}^n \\frac{1}{m}$ is the $n$-th term on the harmonic series.\n",
    "\n",
    "Since the harmonic series diverges, $\\sum_{m \\geq 0} m p_M(m) = \\infty$.\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$$ \\sum_{m=-n}^{-1} m p_M(m) = -\\sum_{m=1}^n m p_M(m) = -\\frac{1}{2} \\left( H_n - 1 \\right) $$\n",
    "\n",
    "so $\\sum_{m < 0} m p_M(m) = -\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The sum after $3n$ terms in the provided order is the sum of the first $2n$ positive terms and the first $n$ negative terms:\n",
    "\n",
    "$$ \\sum_{m=1}^{2n} m p_M(m) + \\sum_{m=-n}^{-1} m p_M(m) = \\frac{1}{2} \\left(H_{2n} - H_n\\right) $$\n",
    "\n",
    "The difference between $H_n$ and $\\ln n$ converges to the Euler-Mascheroni constant $\\gamma$,\n",
    "\n",
    "$$ H_n = \\ln n + \\gamma + \\epsilon_n$$\n",
    "\n",
    "where $\\epsilon_n \\sim 1/2n$ goes to 0 as $n$ goes to infinity.\n",
    "\n",
    "Then,\n",
    "\n",
    "$$ \\frac{1}{2} \\left(H_{2n} - H_n\\right) = \\frac{1}{2} \\left( \\ln 2 + \\epsilon_{2n} - \\epsilon_n \\right) $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\frac{1}{2} \\left(H_{2n} - H_n\\right) = \\frac{\\ln 2}{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**\n",
    "\n",
    "By an analogous argument, adding up the first $kn$ positive terms and the first $n$ negative terms gives us:\n",
    "\n",
    "$$ \\sum_{m=1}^{kn} m p_M(m) + \\sum_{m=-n}^{-1} m p_M(m) = \\frac{1}{2} \\left(H_{kn} - H_n\\right) $$\n",
    "\n",
    "And we have\n",
    "\n",
    "$$ \\frac{1}{2} \\left(H_{kn} - H_n\\right) = \\frac{1}{2} \\left( \\ln k + \\epsilon_{kn} - \\epsilon_n \\right) $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\frac{1}{2} \\left(H_{kn} - H_n\\right) = \\frac{\\ln k}{2} $$\n",
    "\n",
    "The exercise's point is then to show that we can sum up the terms in an an order to approach an arbitrarily large value (by e.g. picking $k$ such that $\\ln k / 2$ is sufficiently large)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.9 (Proof of Theorem 1.4.1)**.  The bounds on the binomial in this theorem are based on the *Stirling bounds*.  These say that for all $n \\geq 1$, $n!$ is upper and lower bound by\n",
    "\n",
    "$$ \\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n < n! < \\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n e^{1 / 12 n} \\tag{1.99}$$\n",
    "\n",
    "The ratio $\\sqrt{2 \\pi n} \\left( n/e \\right)^n / n!$ of the first two terms is monotonically increasing with $n$ toward the limit 1, and the ratio $\\sqrt{2 \\pi n} \\left( n/e \\right)^n \\exp (1 / 12n) / n!$ is monotonically decreasing toward 1.  The upper bound is more accurate, but the lower bound is simpler and known as the Stirling approximation.  See [8] for proofs and further discussion of the above facts.\n",
    "\n",
    "**(a)** Show from (1.99) and from the above monotone property that \n",
    "\n",
    "$$ \\binom{n}{k} < \\sqrt{\\frac{n}{2 \\pi k (n - k)}} \\frac{n^n}{k^k (n - k)^{n - k}} $$\n",
    "\n",
    "Hint: First show that $n! / k! < \\sqrt{n / k} \\; n^n k^{-k} e^{-n+k}$ for $k < n$.\n",
    "\n",
    "**(b)** Use the result of (a) to upper bound $p_{S_n}(k)$ by\n",
    "\n",
    "$$ p_{S_n}(k) < \\sqrt{\\frac{n}{2 \\pi k (n - k)}} \\frac{p^k (1 - p)^{n - k} n^n}{k^k (n - k)^{n - k}} $$\n",
    "\n",
    "Show that this is equivalent to the upper bound in Theorem 1.41.\n",
    "\n",
    "**(c)** Show that\n",
    "\n",
    "$$ \\binom{n}{k} > \\sqrt{\\frac{n}{2 \\pi k (n - k)}} \\frac{n^n}{k^k (n - k)^{n - k}} \\left[ 1 - \\frac{n}{12k (n - k)} \\right] $$\n",
    "\n",
    "**(d)** Derive the lower bound in Theorem 1.41.\n",
    "\n",
    "**(e)** Show that $D(\\tilde{p} \\Vert p) = \\tilde{p} \\ln(\\tilde{p} / p) + (1 - \\tilde{p}) \\ln [ (1 - \\tilde{p}) / (1 - p) ]$ is 0 at $\\tilde{p} = p$ and non-negative elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  Since the ratio between the first two terms on the Stirling bound is monotonically increasing, for $k < n$ we have:\n",
    "\n",
    "$$ \\sqrt{2 \\pi k} \\left( \\frac{k}{e} \\right)^k \\frac{1}{k!} < \\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n \\frac{1}{n!} $$\n",
    "\n",
    "which can be rewritten as\n",
    "\n",
    "$$ \\frac{n!}{k!} < \\sqrt{\\frac{n}{k}} n^n k^{-k}e^{-n+k}$$\n",
    "\n",
    "By using the inverse of Stirling lower bound for $n - k$, we get\n",
    "\n",
    "$$ \\frac{1}{(n - k)!} < \\sqrt{\\frac{1}{2 \\pi (n - k)}}\\frac{e^{n - k}}{(n - k)^{n-k}}$$\n",
    "\n",
    "Multiplying the last two inequalities, we get\n",
    "\n",
    "$$ \\binom{n}{k} = \\frac{n!}{k! (n - k)!} < \\sqrt{\\frac{n}{2 \\pi k (n - k)}} \\frac{n^n}{k^k (n - k)^{n-k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The upper bound in Theorem 1.41 is\n",
    "\n",
    "$$ p_{S_n}(\\tilde{p}n) < \\sqrt{\\frac{1}{2 \\pi n \\tilde{p} (1 - \\tilde{p})}} \\exp [- n D(\\tilde{p} \\Vert p)]$$\n",
    "\n",
    "where $\\tilde{p} = k / n$. \n",
    "\n",
    "The PMF for the binomial distribution is\n",
    "\n",
    "$$ p_{S_n}(k) = \\binom{n}{k} p^k (1 - p)^{n - k} $$\n",
    "\n",
    "so the inequality follows by multiplying both sides of the result in (a) by $p^k (1 - p)^{n - k}$:\n",
    "\n",
    "$$ p_{S_n}(k) = \\binom{n}{k}p^k (1 - p)^{n - k}  < \\sqrt{\\frac{n}{2 \\pi k (n - k)}} \\frac{p^k (1 - p)^{n - k} n^n}{k^k (n - k)^{n-k}}$$\n",
    "\n",
    "But\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\exp [- n D(\\tilde{p} \\Vert p)] &= \\exp \\left\\{ -n\\tilde{p} \\ln \\left( \\frac{\\tilde{p}}{p} \\right) - n(1 - \\tilde{p}) \\ln \\left( \\frac{1 - \\tilde{p}}{1 - p} \\right) \\right\\}  \\\\\n",
    "&= \\exp \\left\\{ -k \\ln \\left( \\frac{k}{np} \\right) - (n - k) \\ln \\left( \\frac{n - k}{n(1 - p)} \\right) \\right\\} \\\\\n",
    "&= \\left(\\exp \\left\\{ \\ln \\left( \\frac{k}{np} \\right) \\right\\}\\right)^{-k}  \\left( \\exp \\left\\{ \\ln \\left( \\frac{n - k}{n(1 - p)} \\right) \\right\\}  \\right)^{-n-k} \\\\\n",
    "&= \\frac{n^k p^k}{k^k} \\frac{n^{n - k} (1 - p)^{n - k}}{(n - k)^{n - k}} \\\\\n",
    "&= \\frac{p^k (1 - p)^{n - k} n^n}{k^k (n - k)^{n - k}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\sqrt{\\frac{1}{2 \\pi n \\tilde{p} (1 - \\tilde{p})}} \\exp [- n D(\\tilde{p} \\Vert p)] = \\sqrt{\\frac{n}{2 \\pi k (n - k)}} \\frac{p^k (1 - p)^{n - k} n^n}{k^k (n - k)^{n - k}} $$\n",
    "\n",
    "therefore the inequality we obtained is equivalent to the upper bound of Theorem 1.41."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**.  Let's do a proof similar to the one in (a), but using the fact that the ratio between the upper bound in the Stirling bound is monotonically decreasing; for $k < n$,\n",
    "\n",
    "$$\\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n e^{1 / 12n} \\frac{1}{n!} < \\sqrt{2 \\pi k} \\left( \\frac{k}{e} \\right)^k e^{1 / 12k} \\frac{1}{k!}$$\n",
    "\n",
    "Rearranging the terms, we get a lower bound for $n! / k!$,\n",
    "\n",
    "$$ \\frac{n!}{k!} > \\sqrt{\\frac{n}{k}} n^n k^{-k} \\exp \\left\\{ -n + \\frac{1}{12n} + k - \\frac{1}{12k}\\right\\} $$\n",
    "\n",
    "By using the inverse of the Stirling upper bound for $n - k$, we get\n",
    "\n",
    "$$ \\frac{1}{(n - k)!} > \\sqrt{\\frac{1}{2 \\pi (n - k)}}\\frac{1}{(n - k)^{n-k}} \\exp \\left\\{ (n - k) - \\frac{1}{12(n - k)}\\right\\}$$\n",
    "\n",
    "Multiplying these inequalities, we get\n",
    "\n",
    "$$ \\binom{n}{k} = \\frac{n!}{k! (n - k)!} > \\sqrt{\\frac{n}{2 \\pi k (n - k)}} \\frac{n^n}{k^k (n - k)^{n - k}} \\exp \\left\\{ \\frac{1}{12n} - \\frac{1}{12k} - \\frac{1}{12(n - k)}\\right\\} $$\n",
    "\n",
    "In order the obtain the result, now it suffices to show that this is a stronger inequality, that is,\n",
    "\n",
    "$$ \\exp \\left\\{ \\frac{1}{12n} - \\frac{1}{12k} - \\frac{1}{12(n - k)} \\right\\} \\geq 1 - \\frac{n}{12k (n - k)}$$\n",
    "\n",
    "We have $\\exp x < 1 - x$ for $x < 0$, since $f(x) = e^x + x$ has value $f(0) = 1$ and $f$ is monotonically increasing.  Then, since $k < n$, we have\n",
    "\n",
    "$$ \\frac{1}{12n} - \\frac{1}{12k} - \\frac{1}{12(n - k)} < 0 $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\exp \\left\\{ \\frac{1}{12n} - \\frac{1}{12k} - \\frac{1}{12(n - k)} \\right\\} < \\frac{1}{12n} - \\frac{1}{12k} - \\frac{1}{12(n - k)} $$\n",
    "\n",
    "It then will suffice to prove the even stronger result that\n",
    "\n",
    "$$\n",
    "\\frac{1}{12n} - \\frac{1}{12k} - \\frac{1}{12(n - k)} < 1 - \\frac{n}{12k (n - k)}\n",
    "$$\n",
    "\n",
    "which is true, since it is equivalent to $\\frac{1 - 12n}{12n} < 0$ and we have $n > 1/12$.\n",
    "\n",
    "Therefore, the desired (weaker) upper bound holds,\n",
    "\n",
    "$$ \\binom{n}{k} > \\sqrt{\\frac{n}{2 \\pi k (n - k)}} \\frac{n^n}{k^k (n - k)^{n - k}} \\left[ 1 - \\frac{n}{12k (n - k)} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  The upper bound in Theorem 1.41 is\n",
    "\n",
    "$$ p_{S_n}(\\tilde{p}n) > \\left(1 - \\frac{1}{12n\\tilde{p}(1 - \\tilde{p})} \\right) \\sqrt{\\frac{1}{2 \\pi n \\tilde{p} (1 - \\tilde{p})}} \\exp [- n D(\\tilde{p} \\Vert p)]$$\n",
    "\n",
    "By multiplying the bound of (c) by $p^k (1-p)^{n-k}$, we get:\n",
    "\n",
    "$$ p_{S_n}(k) = \\binom{n}{k} p^k (1 - p)^{n-k} > \\sqrt{\\frac{n}{2 \\pi k (n - k)}} \\frac{p^k (1 - p)^{n-k} n^n}{k^k (n - k)^{n - k}} \\left[ 1 - \\frac{n}{12k (n - k)} \\right] $$\n",
    "\n",
    "But, if $\\tilde{p} = k / n$, then\n",
    "\n",
    "$$ \\sqrt{\\frac{n}{2 \\pi k (n - k)}} \\left[ 1 - \\frac{n}{12k (n - k)} \\right] = \\left(1 - \\frac{1}{12n\\tilde{p}(1 - \\tilde{p})} \\right) \\sqrt{\\frac{1}{2 \\pi n \\tilde{p} (1 - \\tilde{p})}} $$\n",
    "\n",
    "and, as we proved in (b) that \n",
    "\n",
    "$$ \\exp [- n D(\\tilde{p} \\Vert p)] = \\frac{p^k (1 - p)^{n - k} n^n}{k^k (n - k)^{n - k}} $$\n",
    "\n",
    "the desired result follows by multiplying the last two equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**  This result is a particular case of Gibbs' inequality, which states that the Kullback-Leibler divergence, or relative entropy, is non-negative:\n",
    "\n",
    "$$ D_{KL}(P \\Vert Q) = \\sum_i p_i \\ln \\frac{p_i}{q_i} \\geq 0 $$\n",
    "\n",
    "where $P, Q$ are probability distributions, with $\\sum_i p_i = \\sum_i q_i = 1$.\n",
    "\n",
    "In this problem, we have the (negative) binary KL divergence, with $(p_1, p_2) = (\\tilde{p}, (1 - \\tilde{p})$ and $(q_1, q_2) = (p, 1 - p)$.\n",
    "\n",
    "To demonstrate Gibbs' inequality, note that $\\ln$ is strictly concave, so using Jensen's inequality we have\n",
    "\n",
    "$$ -D_{KL}(P \\Vert Q) = \\sum_i p_i \\ln \\frac{q_i}{p_i} \\leq \\ln \\sum_i p_i \\frac{q_i}{p_i} = \\ln \\sum_i q_i = 0$$\n",
    "\n",
    "with equality holding only when\n",
    "\n",
    "$$ \\frac{q_1}{p_1} = \\frac{q_2}{p_2} = \\cdots $$\n",
    "\n",
    "which in our case implies\n",
    "\n",
    "$$ \\frac{p}{\\tilde{p}} = \\frac{1 - p}{1 - \\tilde{p}} $$\n",
    "\n",
    "or $\\tilde{p} = p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.10**.  Let $X$ be a ternary random variable taking on the three values 0, 1, 2 with probabilities $p_0, p_1, p_2$, respectively.  Find the median of $X$ for each of the three cases below.\n",
    "\n",
    "**(a)** $p_0 = 0.2, p_1 = 0.4, p_2 = 0.4$.\n",
    "\n",
    "**(b)** $p_0 = 0.2, p_1 = 0.2, p_2 = 0.6$.\n",
    "\n",
    "**(c)** $p_0 = 0.2, p_1 = 0.3, p_2 = 0.5$.\n",
    "\n",
    "Note 1: The median is not unique in (c).  Find the interval of the values that are medians. \n",
    "\n",
    "Note 2:  Some people force the median to be distinct by defining it as the midpoint of the interval satisfying the definition given here.\n",
    "\n",
    "**(d)**  Now suppose that $X$ is non-negative and continuous with the density $f_X(x) = 1$ for $0 \\leq x \\leq 0.5$ and $f_X(x) = 0$ for $0.5 \\leq x \\leq 1$.  We know that $f_X(x)$ is positive for all $x > 1$, but it is otherwise unknown.  Find the median or interval of medians.\n",
    "\n",
    "The median is sometimes (incorrectly) defined as that $\\alpha$ for which $\\text{Pr} \\left\\{ X > \\alpha \\right\\} = \\text{Pr} \\left\\{ X < \\alpha \\right\\}$.  Show that it is possible for no such $\\alpha$ to exist.  Hint: look at the examples above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** The unique median is $\\alpha = 1$, since $\\text{Pr}\\{X \\leq 1\\} = 0.6 \\geq 1/2$ and $\\text{Pr}{X \\geq 1} = 0.8 \\geq 1/2$.  These criteria do not apply for $\\alpha \\neq 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** The unique median is $\\alpha = 2$, since $\\text{Pr}\\{X \\leq 2\\} = 1.0 \\geq 1/2$ and $\\text{Pr}\\{X \\geq 1\\} = 0.6 \\geq 1/2$.  These criteria do not apply for $\\alpha \\neq 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** For $\\alpha \\in [1, 2]$ the median condition applies: $\\text{Pr}\\{X \\leq \\alpha\\} \\geq 0.5$ and $\\text{Pr}\\{X \\geq \\alpha\\} \\geq 0.5$.  The median condition does not otherwise apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  By construction, $\\text{Pr}\\{X \\geq \\alpha\\} \\geq 0.5$ for $\\alpha \\geq 0.5$, so all medians must satisfy this condition.  Additionally, $\\text{Pr}\\{X \\geq \\alpha\\} > 0.5$ if $x > 1$, so $\\alpha$ must satisfy the reverse of this condition.  Therefore, the interval of medians is $[0.5, 1]$.\n",
    "\n",
    "The distribution in (a) is an example of a distribution where the incorrect definition of median yields no values:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c}\n",
    "\\alpha & \\text{Pr}\\{ X > \\alpha \\} & \\text{Pr}\\{ X < \\alpha \\} \\\\\n",
    "\\hline\n",
    "\\alpha < 0 & 1 & 0 \\\\\n",
    "\\alpha = 0 & 0.8 & 0 \\\\\n",
    "0 < \\alpha < 1 & 0.8 & 0.2 \\\\\n",
    "\\alpha = 1 & 0.4 & 0.2 \\\\\n",
    "1 < \\alpha < 2 & 0.4 & 0.6 \\\\\n",
    "\\alpha = 2 & 0 & 0.6 \\\\\n",
    "\\alpha > 2 & 0 & 1\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.11**. \n",
    "\n",
    "**(a)** For any given random variable $Y$, express $\\text{E}[|Y|]$ in terms of $\\int_{y < 0} F_Y(y) dy$ and $\\int_{y\\geq 0} F_Y^c(y) dy$.  Hint: Review the argument in Figure 1.4.\n",
    "\n",
    "**(b)** For some random variable $X$ with $\\text{E}[|X|] < \\infty$, let $Y = X - \\alpha$.  Using (a), show that\n",
    "\n",
    "$$ \\text{E}[|X - \\alpha|] = \\int_\\alpha^\\infty F_X(x) dx + \\int_{-\\infty}^\\alpha F_X^c(x) dx$$\n",
    "\n",
    "**(c)** Show that $\\text{E}[|X - \\alpha|]$ is minimized over $\\alpha$ by choosing $\\alpha$ to be the median of $X$.  Hint:  Both the easy way and the most instructive way to do this is to use a graphical argument involving shifting Figure 1.4.  Be careful to show that when the median is an interval, all points in this interval achieve the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** We have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{E}[|Y|] &= \\int |y| f_Y(y) dy \\\\\n",
    "&= \\int_{y \\geq 0} y f_Y(y) dy + \\int_{y < 0} -y f_Y(y) dy \\\\\n",
    "&= \\int_0^\\infty \\left( \\int_0^y dz \\right) f_Y(y) dy + \\int_{-\\infty}^0 \\left( \\int_y^0 dz \\right) f_Y(y) dy \\\\\n",
    "&= \\int_0^\\infty \\left(\\int_z^\\infty f_Y(y) dy \\right) dz + \\int_{-\\infty}^0 \\left(\\int_{-\\infty}^z f_Y(y) dy \\right) dz \\\\\n",
    "&= \\int_0^\\infty F_Y^c(z) dz + \\int_{-\\infty}^0 F_Y(z) dz \\\\\n",
    "&= \\int_{y \\geq 0} F_Y^c(y) dy + \\int_{y < 0} F_Y(y) dy\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "where we performed a integration order change over points $(y, z)$ by a similar argument as in exercise 1.6:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\{ (y, z) : y \\in [0, \\infty), z \\in [0, y] \\} &= \\{ (y, z) : z \\in [0, \\infty), y \\in [z, \\infty) \\} \\\\\n",
    "\\{ (y, z) : y \\in (-\\infty, 0), z \\in [y, 0] \\} &= \\{ (y, z) : z \\in [-\\infty, 0), y \\in (-\\infty, z) \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and we used the fact that the integration from negative infinity to $z$ of the density function is the CDF, $ \\int_{y \\leq z} f_Y(y) dy = F_y(z)$, while the integration from $z$ to plus infinity is the complement of the CDF, $\\int_{y > z} f_Y(y) dy = F_Y^c(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "By definition, $Y = X - \\alpha$, so the CDF of $Y$ can be built from the CDF of $X$:\n",
    "\n",
    "$$ F_Y(x) = F_X(x - \\alpha) $$\n",
    "\n",
    "From (a),\n",
    "\n",
    "$$ \\text{E}[|Y|] = \\int_{x \\geq 0} F_Y^c(x) dx + \\int_{x < 0} F_Y(x) dx $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\text{E}[|X - \\alpha|] = \\int_{x \\geq 0} F_X^c(x - \\alpha) dx + \\int_{x < 0} F_X(x - \\alpha) dx = \\int_{x \\geq \\alpha} F_X^c(x) dx + \\int_{x < \\alpha} F_X(x) dx $$\n",
    "\n",
    "(note that the exercise has a typo:  it flips around the  CDF and its complement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**\n",
    "\n",
    "The intuitive argument is that changing the choice of $\\alpha$ will \"move points\" between the two integrands, causing them to be counted as $F_X(x)$ or $F_X^c(x)$.  Somewhat more formally, considering the function $g(a) = \\text{E}[|X - a|]$, a change in variable from $a$ to $a + \\Delta a$ will increase the result by\n",
    "\n",
    "$$ g(a + \\Delta a) - g(a) = \\int_a^{a + \\Delta a} F_X^c(x) - F_X(x) \\; dx $$\n",
    "\n",
    "while a change in variable from $a$ to $\\Delta a$ will increase the result by\n",
    "\n",
    "$$ g(a - \\Delta a) - g(a) = \\int_{a - \\Delta a}^a F_X(x) - F_X^c(x) \\; dx $$\n",
    "\n",
    "Therefore, if $F_X(\\alpha) = \\text{Pr}\\{X \\leq \\alpha\\} > 1/2$, we have $F_X^c(\\alpha) < 1/2$, and the value of $g(a)$ can be reduced by shifting $a$ down.  Similarly, if $F_X^c(\\alpha) = \\text{Pr}\\{X \\leq \\alpha\\} > 1/2$, we have $F_X(\\alpha) < 1/2$, and we can reduce the value of $g(a)$ by shifting $a$ up.  This minimization step is only not possible if\n",
    "\n",
    "$$ \\text{Pr}\\{X \\leq \\alpha\\} \\geq 1/2, \\quad \\text{Pr}\\{X \\leq \\alpha\\} \\geq 1/2 $$\n",
    "\n",
    "which is to say, if $\\alpha$ lies in the median interval.  Therefore the minimum value for $g$ is obtained at the median interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.12**.  Let $X$ be a random variable with CDF $F_X(x)$.  Find the CDF of the following random variables:\n",
    "\n",
    "**(a)** The maximum of $n$ IID random variables, each with CDF $F_X(x)$.\n",
    "\n",
    "**(b)** The minimum of $n$ IID random variables, each with CDF $F_X(x)$.\n",
    "\n",
    "**(c)** The difference of the random variables defined in (a) and (b); assume $X$ has a density $f_X(x)$.  Hint:  Let $M_+$ and $M_-$ be the maximum and minimum respectively of the $X_i$.  First, conditional on $X_1 = x$, find the joint probability that $X_1 = M_+$ and $M_+ - M_- \\leq r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**.  Let $A_n = \\max \\{ X_1, X_2, \\dots, X_n \\}$, where $X_i \\sim F_X$ and the $X_i$'s are IID.  Then:\n",
    "\n",
    "$$ F_{A_n}(x) = \\text{Pr}\\{ A_n \\leq x \\} = \\prod_{i=1}^n \\text{Pr} \\{ X_i \\leq x \\} = \\prod_{i=1}^n F_X(x) = F_X(x)^n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Let $B_n = \\min \\{ X_1, X_2, \\dots, X_n \\}$, where $X_i \\sim F_X$ and the $X_i$'s are IID.  Then:\n",
    "\n",
    "$$ F^c_{B_n}(x) = \\text{Pr}\\{ B_n > x \\} = \\prod_{i=1}^n \\text{Pr}\\{ X_i > x \\} = \\prod_{i=1}^n F_X^c(x) = (1 - F_X(x))^n $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ F_{B_n}(x) = 1 - (1 - F_X(x))^n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Let $C_n = A_n - B_n$, as defined in (a) and (b):  $A_n = \\max \\{ X_1, X_2, \\dots, X_n \\}$, $B_n = \\min \\{ X_1, X_2, \\dots, X_n \\}$, where $X_i \\sim F_X$ and the $X_i$'s are IID.\n",
    "\n",
    "Conditional on $X_1 = x$, the joint probability that $X_1 = A_n$ and $A_n - B_n \\leq r$ is:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Pr}\\{ X_1 = A_n, A_n - B_n \\leq r | X_1 = x \\}  &= \\text{Pr}\\{ (X_i \\leq x \\text{ for } i = 2, \\dots, n),  B_n \\leq x + r\\} \\\\\n",
    "&= F_X(x)^{n - 1} F_{B_n}(x + r) \\\\\n",
    "&= F_X(x)^{n-1}(1 - (1 - F_X(x + r))^n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can then integrate this over the density of $X_1$ to \"remove\" the conditional clause:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Pr}\\{ X_1 = A_n, A_n - B_n \\leq r\\} &= \\int \\text{Pr}\\{ X_1 = A_n, A_n - B_n \\leq r | X_1 = x \\} f_X(x) dx \\\\\n",
    "&= \\int F_X(x)^{n-1}(1 - (1 - F_X(x + r))^n f_X(x) dx\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally, we can obtain the CDF for $C_n = A_n - B_n$ by partitioning the event on the $n$ events for which $X_i = A_n$, $1 \\leq i \\leq n$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F_{C_n}(r) = \\text{Pr}\\{A_n - B_n \\leq r \\} &= \\sum_{i=1}^n \\text{Pr}\\{ X_i = A_n, A_n - B_n \\leq r\\} \\\\\n",
    "&= n \\; \\text{Pr}\\{ X_1 = A_n, A_n - B_n \\leq r\\} \\\\\n",
    "&= \\int n F_X(x)^{n-1}(1 - (1 - F_X(x + r))^n f_X(x) dx\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.13**.  Let $X$ and $Y$ be random variables in some sample space $\\Omega$ and let $Z = X + Y$, i.e., for each $\\omega \\in \\Omega$, $Z(\\omega) = X(\\omega) + Y(\\omega)$.\n",
    "\n",
    "**(a)** Show that the set of $\\omega$ for which $Z(\\omega) = \\pm \\infty$ has probability 0.\n",
    "\n",
    "**(b)** To show that $Z = X + Y$ is a random variable, we must show that for each real number $\\alpha$, the set $\\{ \\omega \\in \\Omega : X(\\omega) + Y(\\omega) \\leq \\alpha \\}$ is an event.  We proceed indirectly.  For an arbitrary positive integer $n$ and an arbitrary integer $k > 0$, let $B(n, k) = \\{ \\omega : X(\\omega) < k / n \\} \\cap \\{ Y(\\omega) \\leq \\alpha + (1 - k)/n \\}$.  Let $D(n) = \\cup_k B(n, k)$ and show that $D(n)$ is an event.\n",
    "\n",
    "**(c)** On a two-dimensional sketch for a given $\\alpha$, show the values of $X(\\omega)$ and $Y(\\omega)$ for which $\\omega \\in D(n)$.  Hint:  This set of values should be bounded by a staircase function.\n",
    "\n",
    "**(d)** Show that\n",
    "\n",
    "$$ \\{ \\omega: X(\\omega) + Y(\\omega) \\leq \\alpha \\} = \\cap_n D(n) $$\n",
    "\n",
    "Explain why this shows that $Z = X + Y$ is a random variable.\n",
    "\n",
    "**(e)** Explain why (d) implies that if $Y = X_1 + X_2 + \\cdots + X_n$ and if $X_1, X_2, \\dots, X_n$ are random variables, then $Y$ is a random variable.  Hint: only one or two lines of explanation are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  By construction, if there is a $\\omega$ for which $Z(\\omega) = \\pm \\infty$, then $X(\\omega) = \\pm \\infty$ or $Y(\\omega) = \\pm \\infty$.  But since $X$ and $Y$ are random variables, the probability of such events is 0, and so the probability of the union of these events must be zero, therefore $\\text{Pr}\\{Z = \\pm \\infty\\} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  \n",
    "\n",
    "1.  Since $X$ is a random variable, $B_X(n, k) = \\{ \\omega : X(\\omega) < k/n \\}$ must be an event.\n",
    "2.  Since $Y$ is a random variable, $B_Y(n, k) = \\{ \\omega : Y(\\omega) < \\alpha + (1 - k)/n \\}$ must be an event.  \n",
    "3.  Since $B_X(n, k)$ and $B_Y(n, k)$ are events, their intersection $B(n, k)$ must be an event.  In general, if $A$ and $B$ are events, their intersection must be an event:\n",
    "    - Their complements $A^c$, $B^c$ must be events (axiom of events #3)\n",
    "    - The union of their complements must be an event, $A^c \\cup B^c$ (axiom of events #2)\n",
    "    - The complement of their intersection must be an event, $(A \\cap B)^c = A^c \\cup B^c$ (De Morgan)\n",
    "    - The complement of the complement of their intersection, or just their intersection, must be an event (axiom of events #3):  $((A \\cap B)^c)^c = A \\cap B$\n",
    "    \n",
    "4.  Since each $B(n, k)$ is an event, their union $D(n) = \\cup_k B(n, k)$ must be an event (axiom of events #2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "alpha = 0.3\n",
    "n = 10\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "neg_infty = -1\n",
    "\n",
    "for k in np.arange(1, n+4, step=1):\n",
    "    x = [neg_infty, k/n, k/n, neg_infty]\n",
    "    y = [neg_infty, neg_infty, alpha + (1-k)/n, alpha + (1-k)/n]\n",
    "    plt.fill(x, y, alpha=0.5)\n",
    "    \n",
    "plt.xlabel(r'$X(\\omega)$')\n",
    "plt.ylabel(r'$Y(\\omega)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**\n",
    "\n",
    "We have:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "&\\omega \\in \\cap_n D(n)  \\\\\n",
    "\\Longleftrightarrow\\;& \\forall n \\; \\omega \\in D(n) \\\\\n",
    "\\Longleftrightarrow\\;& \\forall n \\; \\exists k \\text{ s.t. } \\omega \\in B(n, k) \\\\\n",
    "\\Longleftrightarrow\\;& \\forall n \\; \\exists k \\text{ s.t. } X(\\omega) < k/n, Y(\\omega) < \\alpha + (1 - k)/n \\\\\n",
    "\\Longrightarrow\\;& \\forall n \\; \\exists k \\text{ s.t. } X(\\omega) + Y(\\omega) < \\alpha \\\\\n",
    "\\Longrightarrow\\;& \\omega \\in \\{ \\omega : X(\\omega) + Y(\\omega) < \\alpha \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and so $$ \\cap_n D(n) \\subseteq \\{ \\omega : X(\\omega) + Y(\\omega) < \\alpha \\} $$\n",
    "\n",
    "In the other direction, note that for any given positive integer $n$ and any $\\omega$, if we pick a positive integer $k > \\max \\{ \\lceil n X(\\omega) \\rceil, 1\\}$, then $X(\\omega) < k / n$.  So:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\omega \\in \\{ \\omega : X(\\omega) + Y(\\omega) < \\alpha \\} \\land \\forall n \\; \\exists k \\text{ s.t. } X(\\omega) < n/k \\\\\n",
    "\\Longrightarrow\\;& \\forall n \\; \\exists k \\text{ s.t. } X(\\omega) < k/n, Y(\\omega) < \\alpha + (1 - k)/n \\\\\n",
    "\\Longleftrightarrow\\;& \\forall n \\; \\exists k \\text{ s.t. } \\omega \\in B(n, k) \\\\\n",
    "\\Longleftrightarrow\\;& \\forall n \\; \\omega \\in D(n) \\\\\n",
    "\\Longleftrightarrow\\;& \\omega \\in \\cap_n D(n)  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and so $$ \\cap_n D(n) \\supseteq \\{ \\omega : X(\\omega) + Y(\\omega) < \\alpha \\} $$\n",
    "\n",
    "Given we proven that $\\cap_n D(n)$ is both a subset and a superset of $\\{ \\omega : X(\\omega) + Y(\\omega) < \\alpha \\}$, we get the desired result,\n",
    "\n",
    "$$ \\cap_n D(n) = \\{ \\omega : X(\\omega) + Y(\\omega) < \\alpha \\} $$\n",
    "\n",
    "Since each $D(n)$ is an event, then by the axiom of events #2 and De Morgan's statement their intersection $\\cap_n D(n)$ is also an event.  This means that $ \\{ \\omega : X(\\omega) + Y(\\omega) < \\alpha \\} = \\{ \\omega : Z(\\omega) < \\alpha \\}$ is also an event.  Since we can do this construction for any $\\alpha$, then $Z$ satisfies the second property of random variables.  The first property was proved in (a), and the third property follows from the axiom of event #2 -- therefore $Z$ is a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**  The result follows from induction on the number of variables -- if $Y_n = X_1 + \\cdots + X_n$ is a random variable, then $Y_{n + 1} = Y_n + X_{n+1}$ is a sum of random variables, and by (d) is also a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.14**.\n",
    "\n",
    "**(a)** Let $X_1, X_2, \\dots, X_n$ be random variables with expected values $\\overline{X}_1, \\dots, \\overline{X}_n$.  Show that $\\text{E}[X_1 + \\cdots + X_n] = \\overline{X}_1 + \\cdots + \\overline{X}_n$.  You may assume the random variables have a joint density function, but do not assume that the random variables are independent.\n",
    "\n",
    "**(b)** Now assume that $X_1, \\dots, X_n$ are statistically independent and show that the expected value of the product is equal to the product of the expected values.\n",
    "\n",
    "**(c)** Again assuming that $X_1, \\dots, X_n$ are statistically independent, show that the variance of the sum is equal to the sum of variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**.  Assuming a joint density function $f(x_1, \\dots, x_n)$, and marginal density functions $f_{X_i}(x_i)$, we have:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{E}\\left[ \\sum_{i=1}^n X_i \\right] &= \\int_{X_1} \\int_{X_2} \\cdots \\int_{X_n} \\sum_{i=1}^n x_i f(x_1, \\dots, x_i) dx_1 \\dots dx_n \\\\\n",
    "&= \\sum_{i=1}^n \\int_{X_1} \\int_{X_2} \\cdots \\int_{X_n} x_i f(x_1, \\dots, x_i) dx_1 \\dots dx_n \\\\\n",
    "&= \\sum_{i=1}^n \\int_{X_i} x_i f_{X_i}(x_i) dx_i \\\\\n",
    "&= \\sum_{i=1}^n \\overline{X}_i\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Statistical independence implies that we can multiply the marginal density functions to obtain the joint density function,\n",
    "\n",
    "$$ f(x_1, \\dots, x_i) = \\prod_{i=1}^n f_{X_i}(x_i) $$\n",
    "\n",
    "Then, we have:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{E}\\left[ \\prod_{i=1}^n X_i \\right] &= \\int_{X_1} \\int_{X_2} \\cdots \\int_{X_n} \\left(\\prod_{i=1}^n x_i\\right) f(x_1, \\dots, x_i) dx_1 \\dots dx_n \\\\\n",
    "&= \\int_{X_1} \\int_{X_2} \\cdots \\int_{X_n} \\left(\\prod_{i=1}^n x_i f_{X_i}(x_i) \\right) dx_1 \\dots dx_n \\\\\n",
    "&= \\prod_{i=1}^n \\int_{X_i} x_i f_{X_i}(x_i) dx_i \\\\\n",
    "&= \\prod_{i=1}^n \\overline{X}_i\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  We have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Var}\\left[ \\sum_{i=1}^n X_i \\right] &= \\text{E}\\left[ \\left(\\sum_{i=1}^n X_i \\right)^2\\right] - \\text{E}\\left[ \\sum_{i=1}^n X_i \\right]^2 \\\\\n",
    "&= \\text{E}\\left[ \\sum_{i=1}^n X_i^2 + \\sum_{i=1}^n \\sum_{j=1; j \\neq i}^n X_i X_j \\right] - \\left( \\sum_{i=1}^n \\text{E}[X_i] \\right)^2 \\\\\n",
    "&= \\sum_{i=1}^n \\text{E}[X_i^2] + \\sum_{i=1}^n \\sum_{j=1; j \\neq i}^n \\text{E}[X_i X_j] -  \\sum_{i=1}^n \\text{E}[X_i]^2 - \\sum_{i=1}^n \\sum_{j=1; j \\neq i}^n \\text{E}[X_i] \\text{E}[X_j] \\\\\n",
    "&= \\sum_{i=1}^n \\text{E}[X_i^2] - E[X_i]^2 \\\\\n",
    "&= \\sum_{i=1}^n \\text{Var}[X_i]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we used the fact that statistical independence implies $\\text{E}[X_i X_j] = \\text{E}[X_i] \\text{E}[X_j]$ for $i \\neq j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.15  (Stieltjes integration)**.  \n",
    "\n",
    "**(a)**  Let $h(x) = u(x)$ and $F_X(x) = u(x)$, where $u(x)$ is the unit step, i.e. $u(x) = 0$ for $-\\infty < x < 0$ and $u(x) = 1$ for $x \\geq 0$.  Using the definition of the Stieltjes integral in Footnote 19 on p. 23, show that $\\int_{-1}^1 h(x) dF_X(x)$ does not exist.  Hint: Look at the term in the Riemann sum including $x = 0$ and look at the range of choices for $h(x)$ in that interval.  Intuitively, it might help initially to view $dF_X(x)$ as a unit impulse at $x = 0$.\n",
    "\n",
    "**(b)**  Let $h(x) = u(x - a)$ and $F_X(x) = u(x - b)$, where $a$ and $b$ are in $(-1, +1)$.  Show that $\\int_{-1}^1 h(x) dF_X(x)$ exists if and only if $a \\neq b$.  Show that the integral has the value 1 for $a < b$ and value 0 for $a > b$.  Argue that this result is still valid in the limit of integration over $(-\\infty, \\infty)$.\n",
    "\n",
    "**(c)**  Let $X, Y$ be independent discrete random variables, each with a finite set of possible values.  Show that $\\int_{-\\infty}^\\infty F_X(z - y) dF_Y(y)$, defined as a Stieltjes integral, is equal to the distribution of $Z = X + Y$ at each $z$ other than the possible sample values of $Z$, and is undefined at each sample value of $Z$.  Hint:  Express $F_X$ and $F_Y$ as sums of unit steps.  Note: This failure of Stieltjes integration is not a serious problem;  $F_Z(z)$ is a step function, and the integral is undefined at its points of discontinuity.  We automatically define $F_Z(z)$ at those step values so that $F_Z$ is a CDF (i.e. is continuous from the right).  This problem does not arise if either $X$ or $Y$ is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "The Riemann-Stieltjes integral, abbreviated here as the Stieltjes integral, is denoted as $\\int_a^b h(x) dF_X(x)$.  It is defined as the limit of a generalized Riemman sum, $ \\lim_{\\delta \\rightarrow 0} \\sum_n h(x_n) [F(y_n) - F(y_{n-1})] $, where $\\{y_n : n \\geq 1\\}$ is a sequence of increasing numbers from $a$ to $b$ satisfying $y_n - y_{n-1} \\leq \\delta$ and $y_{n-1} < x_n \\leq y_n$ for all $n$.  The Stieltjes integral is defined to exist over finite limits if the limit exists and is independent of the choices of $\\{ y_n \\}$ and $\\{ x_n \\}$ as $\\delta \\rightarrow 0$.  It exists over infinite limits if it exists over finite lengths and a limit over the integration limits can be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** We are interested in the limit of\n",
    "\n",
    "$$ \\lim_{\\delta \\rightarrow 0} S(x, y) \\quad \\text{where } S(x, y) = \\sum_n u(x_n) [u(y_n) - u(y_{n - 1})] $$\n",
    "\n",
    "Since $u$ is the unit step, the term $[u(y_n) - u(y_{n - 1})]$ is 1 if $y_{n-1} < 0 \\leq y_n$ and 0 otherwise.  Therefore, only one term in the sum is non-zero; the one corresponding to the first $n$ such that $y_{n-1} < 0 \\leq y_n$.  But we can choose the series $\\{ x_n \\}$ to force that value to be 0 or 1:\n",
    "\n",
    "- If $y_n = 4n\\delta - 1, x_n = y_n$, then $ [u(y_n) - u(y_{n - 1})] = 1 $ if and only if $y_{n-1} < 0 \\leq y_n$, or $\\frac{1}{4 \\delta} < n \\leq \\frac{1}{4 \\delta} + 1$, which for sufficiently small $\\delta$ implies $n = \\lfloor \\frac{1}{4 \\delta} + 1 \\rfloor$.  But then $x_n = y_n = 4 \\lfloor \\frac{1}{4 \\delta} + 1 \\rfloor \\delta - 1 \\geq 0$, and so $S(x, y) = u(x_n) = 1$.\n",
    "\n",
    "- If $y_n = 4n\\delta - 1, x_n = \\frac{1}{2}(y_n + y_{n-1})$, again the only non-zero term occurs (with sufficiently small $\\delta$) for $n = \\lfloor \\frac{1}{4 \\delta} + 1 \\rfloor$, but now we have $x_n < 0$, and so $S(x, y) = u(x_n) = 0$.\n",
    "\n",
    "Since we can construct sequences $x, y$ such that $S(x, y)$ take value 0 or take value 1 for any sufficiently small $\\delta$, the limit is not defined, and so the Stieltjes integral does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  If $a = b$, we can shift the integration variable $x$ to $y - a$.  By a result analogous to (a) (shifting all of the $x, y$ variable series by $a$), again we have that the Stieltjes integral does not exist.\n",
    "\n",
    "If $a < b$, then the only non-zero term in $S(x, y)$ occurs with $F(y_n) - F(y_{n-1}) = 1$, or $y_{n-1} < b \\leq y_n$.  For a sufficiently small $\\delta$, this implies $y_{n-1} > a$, so $x_n > a$ and so $h(x_n) = u(x_n - a) = 1$, and the sum $S(x, y)$ is always 1 -- therefore the Stieltjes integral exists and has value 1.\n",
    "\n",
    "If $a > b$, then the only non-zero term in $S(x, y)$ occurs with $F(y_n) - F(y_{n - 1}) = 1$, or $y_{n - 1} < b \\leq y_n$.  For a sufficiently small $\\delta$, this implies $y_n < a$, so $x_n < a$ and so $h(x_n) = u(x_n - a) = 0$, and the sum $S(x, y)$ is always 0 -- therefore the Stieltjes integral exists and has value 0.\n",
    "\n",
    "This result is still valid in the limit of integration over $(-\\infty, \\infty)$ -- there is nothing special about the interval $(-1, +1)$ other than it being a finite interval, and we could define a new sum over sequences $(x', y')$ that gradually stretch the bounds from $(-1, 1)$ to $(-\\infty, \\infty)$ at a rate slower than $\\delta$.  Since the results obtained so far would also hold for these stretched intervals, the result will also hold in the limit of integration over $(-\\infty, \\infty)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  As suggested, we can decompose the CDF of a random variables $X$ with a finite number of discrete values into sums of unit steps at each of its possible values:\n",
    "\n",
    "$$ F_X(x) = \\sum_{i: \\text{Pr}\\{X = \\alpha_i\\} > 0} u(x - \\alpha_i) \\; \\text{Pr}\\{X = \\alpha_i\\} $$\n",
    "\n",
    "or, for simplicity,\n",
    "\n",
    "$$ \n",
    "F_X(x) = \\sum_i u(x - \\alpha_i) p_i\n",
    "\\quad\n",
    "F_Y(y) = \\sum_j u(y - \\beta_j) q_j\n",
    "$$\n",
    "\n",
    "The Stieltjes integral (if it exists) can then have their sums decomposed into Stieltjes integrals of unit steps:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\int_{-\\infty}^\\infty F_X(z - y) d F_Y(y) &= \\lim_{\\delta \\rightarrow 0} \\sum_n F_X(x_n) [F_Y(y_n) - F_Y(y_{n-1})] \\\\\n",
    "&= \\lim_{\\delta \\rightarrow 0} \\sum_n \\left( \\left[ \\sum_i u(x_n - \\alpha_i) p_i \\right] \\left[ \\sum_j (u(y_n - \\beta_j) - u(y_{n-1} - \\beta_j) ) q_j \\right] \\right) \\\\\n",
    "&= \\lim_{\\delta \\rightarrow 0} \\sum_i \\sum_j \\sum_n p_i q_j u(x_n - \\alpha_i) [u(y_n - \\beta_j) - u(y_{n-1} - \\beta_j)] \\\\\n",
    "&= \\sum_i \\sum_j \\lim_{\\delta \\rightarrow 0} p_i q_j u(x_n - \\alpha_i) [u(y_n - \\beta_j) - u(y_{n-1} - \\beta_j)] \\\\\n",
    "&= \\sum_i \\sum_j p_i q_j \\int_{-\\infty}^\\infty u(z - y - \\alpha_i) du(y - \\beta_j)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we have used the fact that, if the integral exists, we can use any sequence of $x, y$ values to compute the limit.\n",
    "\n",
    "If $z$ takes any of the values where there is a unit step, the Stieltjes integral corresponding to that unit step does not exist and all others do, so the overall integral does not exist.  If $z$ does not take one of these values, all Stieltjes integral terms exist, and so the main Stieltjes integral also exists.  Finally, again by construction, the integral takes the value of $F_Z(z)$ wherever it exists, since it corresponds to a sum of probabilities $p_i, q_j$ where $\\alpha_i + \\beta_j \\leq z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.16**.  Let $X_1, X_2, \\dots, X_n$ be a sequence of IID continuous random variables with the common PDF $f_X(x)$; note that $\\text{Pr}\\{X = \\alpha\\} = 0$ for all $\\alpha$ and that $\\text{Pr}\\{ X_i = X_j \\} = 0$ for all $i \\neq j$.  For $n \\geq 2$, define $X_n$ as a *record-to-date* of the sequence if $X_n > X_i$ for all $i < n$.\n",
    "\n",
    "**(a)** Find the probability that $X_2$ is a record-to-date.  Use symmetry to obtain a numerical answer without computation.  A one- or two-line explanation should be adequate.\n",
    "\n",
    "**(b)** Find the probability that $X_n$ is a record-to-date, as a function of $n \\geq 1$.  Again use symmetry.\n",
    "\n",
    "**(c)** Find a simple expression for the expected number of records-to-date that occur over the first $m$ trials for any given integer $m$.  Hint:  Use indicator functions.  Show that this expected number is infinite given the limit $m \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  By symmetry, $\\text{Pr}\\{X_2 > X_1\\} = \\text{Pr}\\{X_2 < X_1\\} = 1/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** By symmetry, $\\text{Pr}\\{X_i = \\max\\{X_1, \\dots, X_n\\}\\}$ assumes the same value for all $1 \\leq i \\leq n$, so the probability that $X_n$ is a record-to-date is $1/n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  The expected number of records-to-date is:\n",
    "\n",
    "$$ \\text{E}\\left[ \\sum_{i=2}^m I(X_i \\text{ is record-to-date}) \\right] = \\sum_{i=2}^m \\text{E}[I(X_i \\text{ is record-to-date})] = \\sum_{i=2}^m\\text{Pr}\\{ X_i \\text{ is record-to-date} \\} = \\sum_{i=1}^m \\frac{1}{m} - 1 = H_m - 1$$\n",
    "\n",
    "that is, the expected number of records-to-date is the $m$-th value of the harmonic series minus 1.  The harmonic series is known to diverge; one intuitive proof is to round down terms to inverse powers of 2.  Note that\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "H_{2^m} &= \\sum_{i=1}^{2^m} \\frac{1}{i} \\geq \\sum_{i=1}^{2^m} \\frac{1}{2^{\\lfloor \\log_2 i \\rfloor}} \\\\\n",
    "&\\geq 1 + 2 \\frac{1}{2} + 4 \\frac{1}{4} + \\cdots + 2^{m - 2} \\frac{1}{2^{m - 2}} \\\\\n",
    "&= m - 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "so terms in the series can be made arbitrarily large (if very slowly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.17 (Continuation of Exercise 1.16)**.  \n",
    "\n",
    "**(a)** Let $N_1$ be the index of the **first** record-to-date in the sequence.  Find $\\text{Pr}\\{N_1 > n\\}$ for each $n \\geq 2$.  Hint:  There is a far simpler way to do this than working from Exercise 1.16(b).\n",
    "\n",
    "**(b)** Show that $N_1$ is a random variable.\n",
    "\n",
    "**(c)** Show that $\\text{E}[N_1] = \\infty$.\n",
    "\n",
    "**(d)** Let $N_2$ be the index of the **second** record-to-date in the sequence.  Show that $N_2$ is a random variable.  Hint:  You need not find the CDF of $N_2$ here.\n",
    "\n",
    "**(e)** Contrast your result in (c) to the result of Exercise 1.16(c) saying that the expected number of records-to-date is infinite over an infinite number of trials.  Note:  This should be a shock to your intuition -- there is an infinite expected wait for the first of an infinite sequence of occurrences, each of which must eventually occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** In the events where $N_1 > n$, then the first sample $X_1$ is the largest sample in the first $n$ samples, $X_1 = \\max \\{ X_1, \\dots, X_n \\}$.  This occurs with probability $1 / n$, so\n",
    "\n",
    "$$ \\text{Pr}\\{N_1 > n\\} = \\frac{1}{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  From part (a), \n",
    "\n",
    "$$ \\text{Pr}\\{ N_1 \\leq n \\} = 1 - \\frac{1}{n} \\quad \\text{for integer } n \\geq 2 $$\n",
    "\n",
    "$N_1$ only assumes integer values greater than or equal to 2, so\n",
    "\n",
    "$$ \\text{Pr}\\{ N_1 \\leq \\alpha \\} = \\begin{cases}\n",
    "0 &\\text{if } \\alpha < 2 \\\\\n",
    "1 - \\frac{1}{\\lfloor \\alpha \\rfloor} &\\text{otherwise}\n",
    "\\end{cases} $$\n",
    "\n",
    "Therefore $N_1$ has corresponding events $\\{ \\omega: N(\\omega) \\leq \\alpha \\}$ for all $\\alpha$, and is a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  The expectation of $N_1$ can be represented as a sum over every possible value:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{E}[N_1] &= \\sum_{n=2}^\\infty n \\;\\text{Pr}\\{N_1 = n\\} \\\\\n",
    "&= \\sum_{n=2}^\\infty n \\left( \\text{Pr}\\{ N_1 \\leq n \\} - \\text{Pr}\\{ N_1 \\leq n - 1 \\} \\right)  \\\\\n",
    "&\\geq \\sum_{n=3}^\\infty n \\left( \\frac{1}{n} - \\frac{1}{n - 1} \\right)  \\\\\n",
    "&= \\sum_{n=2}^\\infty \\frac{1}{n}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is the harmonic series (without the first term), and it diverges, so\n",
    "\n",
    "$$ \\text{E}[N_1] = \\infty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  In the events where $N_2 > n$, then there is a $i$ such that $X_i$ is the largest sample in the first $n$ samples and $X_1$ is the second largest sample.  Therefore $N_2 > n$ is an event with an associated probability, and its complement $N_2 \\leq n$ is also an event with associated probability -- so $N_2$ is a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**  The contrast is that the expected wait time is infinite, but the expected number of events is *also* infinite.  This contradicts a potential intuition that waiting for an event for an infinite expected time would imply a bound on the expected number of events; an infinite number of events can, in fact, be separated by intervals with infinite expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.18 (Another direction from Exercise 1.16)**\n",
    "\n",
    "**(a)** For any given $n \\geq 2$, find the probability that $X_n$ and $X_{n+1}$ are both records-to-date.  Hint:  The idea in Exercise 1.16(b) is helpful here, but the result is not.\n",
    "\n",
    "**(b)** Is the event that $X_n$ is a record-to-date statistically independent of the event that $X_{n+1}$ is a record to date?\n",
    "\n",
    "**(c)** Find the expected number of adjacent pairs of records-to-date over the sequence $X_1, X_2, \\dots$.  Hint:  A helpful fact here is that $1 / (n(n+1)) = 1/n - 1/(n+1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** By symmetry, the probability that the two largest values in the $n+1$ elements $X_1, \\dots, X_{n+1}$ are in two indexes $a$, $b$ is the same for any choice of $a$, $b$:\n",
    "\n",
    "$$ \\text{Pr}\\left\\{ X_a, X_b > X_i \\text{ for all } i\\right\\} = c $$\n",
    "\n",
    "There are $(n+1)n$ such choices, so the desired probability is $1 / n(n+1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The events are statistically independent, since\n",
    "\n",
    "$$ \\text{Pr}\\{ X_n, X_{n + 1} \\text{ are records-to-date} \\} = \\text{Pr}\\{ X_n \\text{ is a record-to-date}\\} \\text{Pr}\\{ X_{n+1} \\text{ is a record-to-date}\\} = \\frac{1}{n(n+1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** The expected number of adjacent pairs of records-to-date in the first $m$ samples is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{E}\\left[ \\sum_{i=2}^m I(X_i, X_{i+1} \\text{ are records-to-date}) \\right] &= \\sum_{i=2}^m \\text{E}[I(X_i, X_{i+1} \\text{ are records-to-date})] \\\\\n",
    "&= \\sum_{i=2}^m\\text{Pr}\\{ X_i, X_{i+1} \\text{ are records-to-date} \\}  \\\\\n",
    "&= \\sum_{i=2}^m \\frac{1}{i(i+1)} \\\\\n",
    "&= \\sum_{i=2}^m \\frac{1}{i} - \\frac{1}{i+1} \\\\\n",
    "&= \\frac{1}{2} - \\frac{1}{m+1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the last equality uses the telescopic sum to cancel out all terms other than the first and the last one.\n",
    "\n",
    "In the limit as $m \\rightarrow \\infty$, this expected value goes to $1/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.19**.\n",
    "\n",
    "**(a)**  Assume that $X$ is a non-negative discrete random variable taking on values $a_1, a_2, \\dots$ and let $Y = h(X)$ for some non-negative function $h$.  Let $b_i = h(a_i)$, $i \\geq 1$ be the $i$-th value taken on by $Y$.  Show that $\\text{E}[Y] = \\sum_i b_i p_Y(b_i) = \\sum_i h(a_i) p_X(a_i)$.  Find an example where $\\text{E}[X]$ exists but $\\text{E}[Y] = \\infty$.\n",
    "\n",
    "**(b)**  Let $X$ be a non-negative continuous random variable with density $f_X(x)$ and let $h(x)$ be differentiable, non-negative, and strictly increasing in $x$.  Let $A(\\delta) = \\sum_{n \\geq 1} h(n \\delta) [F(n \\delta) - F(n \\delta - 1)]$, i.e, $A(\\delta)$ is a $\\delta$-th order approximation to the Stieltjes integral $\\int h(x) dF(x)$.  Show that if $A(1) < \\infty$, then $A(2^{-k}) \\leq A(2^{-(k-1)}) < \\infty$.  Show from this that $\\int h(x) dF(x)$ converges to a finite value.  Note:  This is a very special case, but it can be extended to many cases of interest.  It seems better to consider these convergence questions as required rather than to consider them in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The result for the expectation of $Y = h(X)$ is almost immediate: $ \\text{E}[Y] = \\sum_i b_i p_Y(b_i)$, $b_i = h(a_i)$ and $p_Y(b_i) = p_X(a_i)$, so the result follows.  \n",
    "\n",
    "For an example where $\\text{E}[X]$ exists but $\\text{E}[Y]$ does not, let\n",
    "$a_n = n$, $p_X(a_n) = 2^{-n}$.  Then $\\text{E}[X] = \\sum_{n \\geq 1} n \\cdot 2^{-n} = 1$:\n",
    "\n",
    "$$ \\text{E}[X] = \\sum_{n \\geq 1} n \\cdot 2^{-n} = \\frac{1}{2} \\sum_{n \\geq 1} n \\cdot 2^{-n+1} = \\frac{1}{2} \\left( \\sum_{n \\geq 1} (n - 1) \\cdot 2^{-(n-1)} + \\sum_{n \\geq 1} 2^{-(n-1)} \\right) = \\frac{1}{2} \\left( \\text{E}[X] + 1 \\right) $$\n",
    "\n",
    "from which it follows that $\\text{E}[X] = 1$.\n",
    "\n",
    "Also let $h(x) = 2^x$.  Then\n",
    "\n",
    "$$ \\text{E}[Y] = \\sum_{n \\geq 1} h(n) \\cdot 2^{-n} = \\sum_{n \\geq 1} 2^n \\cdot 2^{-n} = \\sum_{n \\geq 1} 1$$\n",
    "\n",
    "which diverges to $\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  \n",
    "\n",
    "For each $0 < \\delta \\leq 1$, define a random variable $Z_\\delta$ that only assumes non-negative integer discrete values, with  \n",
    "\n",
    "$$\\text{Pr}\\{Z_\\delta = n\\} = \\begin{cases}\n",
    "F(n\\delta) - F(n\\delta - 1) &\\text{if } n \\geq 1 \\\\\n",
    "c_\\delta &\\text{if } n = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "for some constant $c_\\delta \\geq 0$ such that the probabilities sum up to 1.  Note that this is a valid probability mass function definition for $0 < \\delta \\leq 1$, since \n",
    "\n",
    "$$ \\sum_{n \\geq 1} \\text{Pr}\\{ Z_\\delta = n\\} = \\sum_{n \\geq 1} F(n\\delta) - F(n\\delta - 1) \\leq \\sum_{n \\geq 1} F(n\\delta) - F((n - 1)\\delta) \\leq \\left( \\lim_{n \\rightarrow \\infty} F(n \\delta) \\right) - F(0) = 1 - F(0) \\leq 1 $$\n",
    "\n",
    "Then, by construction, \n",
    "\n",
    "$$ \\text{E}[\\delta Z_\\delta] = \\sum_{n \\geq 1} n \\delta \\cdot [F(n \\delta) - F(n\\delta - 1)]$$\n",
    "\n",
    "so\n",
    "\n",
    "$$ \\text{E}[h(\\delta Z_\\delta)] = \\sum_{n \\geq 1} h(n \\delta) [F(n \\delta) - F(n\\delta - 1)] = A(\\delta) $$\n",
    "\n",
    "We want to show that $A(2^{-k}) \\leq A(2^{-(k-1)})$, assuming these quantities exist / are finite.  But since they are expectations, if suffices to show that the values assumed by the corresponding random variables respect that inequality for every singleton event $\\{ \\omega \\}$ in the event space $\\Omega$.  This means if suffices to show that for $0 < \\delta \\leq 1/2$,\n",
    "\n",
    "$$ h(\\delta Z_\\delta(\\omega)) \\leq h((2 \\delta) Z_{2 \\delta}(\\omega)) $$\n",
    "\n",
    "and since $h$ is strictly increasing, it suffices to show that\n",
    "\n",
    "$$ Z_\\delta(\\omega) \\leq 2 Z_{2 \\delta}(\\omega) $$\n",
    "\n",
    "Now, we can explicitly define the function $Z_\\delta : \\mathbb{R} \\rightarrow \\{0, 1, 2, \\dots \\}$ for each $0 < \\delta \\leq 1$,\n",
    "\n",
    "$$ Z_\\delta(\\omega) = \\max \\left\\{ \\left\\lceil \\frac{f_X(\\omega)}{\\delta} \\right\\rceil, 0 \\right\\} $$\n",
    "\n",
    "We define the function this way so that $Z_\\delta(\\omega) = n$ for integer $n > 0$ if and only if\n",
    "\n",
    "$$ \\left\\lceil \\frac{f_X(\\omega)}{\\delta} \\right\\rceil = n $$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$ n\\delta - 1 < f_X(\\omega) \\leq n\\delta $$\n",
    "\n",
    "Therefore, if we assign the probability density $f_X$ to the event space $\\Omega = \\mathbb{R}$, the probability density function of $Z_w$ satisfies the previously given definition.\n",
    "\n",
    "Now, finally, we only need to show that \n",
    "\n",
    "$$ \\max \\left\\{ \\left\\lceil \\frac{f_X(\\omega)}{\\delta} \\right\\rceil, 0 \\right\\} \\leq 2 \\max \\left\\{ \\left\\lceil \\frac{f_X(\\omega)}{2\\delta} \\right\\rceil, 0 \\right\\} $$\n",
    "\n",
    "which has both sides equal to 0 if $f_X(\\omega) \\leq 0$, and is otherwise equivalent to\n",
    "\n",
    "$$ \\left\\lceil \\frac{f_X(\\omega)}{\\delta} \\right\\rceil \\leq 2 \\left\\lceil \\frac{f_X(\\omega)}{2\\delta} \\right\\rceil $$\n",
    "\n",
    "which is true, as in general, for any constant $c > 0$,\n",
    "\n",
    "$$ \\frac{\\left\\lceil c \\right\\rceil}{2} \\leq \\left\\lceil \\frac{c}{2} \\right\\rceil $$\n",
    "\n",
    "Therefore, we have proved that for all $\\omega$, $\\delta Z_\\delta(\\omega) \\leq 2 \\delta Z_{2 \\delta}(\\omega)$, and so \n",
    "\n",
    "$$ A(2^{-k}) = \\text{E}[h(2^{-k} Z_{2^{-k}})] \\leq \\text{E}[h(2 \\cdot 2^{-k} Z_{2 \\cdot 2^{-k}})] = A(2^{-(k-1)})$$ \n",
    "\n",
    "Since all of these are bound by the case $k = 0$, $A(1)$, and since we assume that $A(1)$ converges to a finite value, then all $A(2^{-k})$ converge to a finite value.  Since $g(k) = A(2^{-k})$ is non-increasing, then we must have $\\lim_{\\delta \\rightarrow 0+} A(\\delta) = \\int h(x) dF(x)$  must also converge to a finite value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.20**.\n",
    "\n",
    "**(a)**  Consider a positive, integer-valued random variable whose CDF is given at integer values by\n",
    "\n",
    "$$ F_Y(y) = 1 - \\frac{2}{(y+1)(y+2)} $$\n",
    "\n",
    "Use (1.30) to show that $\\text{E}[Y] = 2$.  Hint:  Note the PMF given in (1.29).\n",
    "\n",
    "**(b)**  Find the PMF of $Y$ and use it to check the value of $\\text{E}[Y]$.\n",
    "\n",
    "**(c)**  Let $X$ be another positive, integer-valued random variable.  Assume its conditional PMF is given by\n",
    "\n",
    "$$ p_{X | Y}(x | y) = \\frac{1}{y} \\quad \\text{for } 1 \\leq x \\leq y$$\n",
    "\n",
    "Find $\\text{E}[X | Y = y]$ and show that $\\text{E}[X] = 3/2$.  Explore finding $p_X(x)$ until you are convinced that using the conditional expectation to calculate $\\text{E}[X]$ is considerably easier than using $p_X(x)$.\n",
    "\n",
    "**(d)**  Let $Z$ be another integer-valued random variable with the conditional PMF\n",
    "\n",
    "$$ p_{Z | Y}(z | y) = \\frac{1}{y^2} \\quad \\text{for } 1 \\leq z \\leq y^2 $$\n",
    "\n",
    "Find $\\text{E}[Z | Y = y]$ for each integer $y \\geq 1$ and find $\\text{E}[Z]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  As in 1.30,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{E}[Y] &= \\sum_{y \\geq 0} (1 - F_Y(y))  \\\\\n",
    "&= \\sum_{y \\geq 0} \\frac{2}{(y+1)(y+2)} \\\\\n",
    "&= 2 \\sum_{y \\geq 0} \\left(\\frac{1}{y + 1} - \\frac{1}{y + 2} \\right) \\\\\n",
    "&= 2 \\left( \\frac{1}{1} - \\frac{1}{2} + \\frac{1}{2} - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{4} + \\cdots \\right)  \\\\\n",
    "&= 2 \\cdot 1 = 2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The PMF of $Y$ is, for integer $y \\geq 0$,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "p_Y(y) &= F_Y(y) - F_Y(y - 1) \\\\\n",
    "&= 1 - \\frac{2}{(y+1)(y+2)} - \\left( 1 - \\frac{2}{y(y+1)} \\right) \\\\\n",
    "&= \\frac{2}{y(y+1)} - \\frac{2}{(y+1)(y+2)} \\\\\n",
    "&= \\frac{4}{y(y+1)(y+2)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{E}[Y] &= \\sum_{y > 0} y p_Y(y) \\\\\n",
    "&= \\sum_{y > 0} \\frac{4}{(y+1)(y+2)} \\\\\n",
    "&= 4 \\sum_{y > 0} \\left( \\frac{1}{y+1} - \\frac{1}{y+2} \\right) \\\\\n",
    "&= 4 \\left( \\frac{1}{2} - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{4} + \\frac{1}{4} - \\frac{1}{5} + \\cdots \\right) \\\\\n",
    "&= 4 \\cdot \\frac{1}{2} = 2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  We have:\n",
    "\n",
    "$$ \\text{E}[X | Y = y] = \\sum_{x = 1}^y x p_{X | Y}(x | y) = \\sum_{x = 1}^y \\frac{x}{y} = \\frac{1}{y} \\frac{y(y+1)}{2} = \\frac{y+1}{2} $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\text{E}[X] = \\text{E}[\\text{E}[X | Y]] = \\text{E}\\left[ \\frac{Y + 1}{2} \\right] = \\frac{\\text{E}[Y] + 1}{2} = \\frac{3}{2} $$\n",
    "\n",
    "Explicitly finding the PMF of $X$ would be more complex:\n",
    "\n",
    "$$ p_X(x) = \\sum_{y > 0} p_{X | Y}(x | y) p_Y(y) = \\sum_{y \\geq x} \\frac{1}{y} \\frac{4}{y(y+1)(y+2)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  We have:\n",
    "\n",
    "$$ \\text{E}[Z | Y = y] = \\sum_{z=1}^{y^2} z p_{Z | Y}(z | y) = \\sum_{z=1}^{y^2} \\frac{z}{y^2} = \\frac{1}{y^2} \\frac{y^2(y^2 + 1)}{2} = \\frac{y^2 + 1}{2} $$\n",
    "\n",
    "But:\n",
    "\n",
    "$$ \\text{E}[Y^2] = \\sum_{y > 0} y^2 p_Y(y) = \\sum_{y > 0} \\frac{4y}{(y+1)(y+2)}> \\sum_{y > 1} \\frac{1}{y+2} = \\infty$$\n",
    "\n",
    "which is to say, the sum diverges and the expectation of $Y^2$ is infinite.\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$ \\text{E}[Z] = \\text{E}[\\text{E}[Z | Y]] = \\text{E}\\left[ \\frac{Y^2 + 1}{2} \\right] = \\frac{\\text{E}[Y^2] + 1}{2} = \\infty $$\n",
    "\n",
    "which is to say the expectation of $Z$ is also infinite."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
