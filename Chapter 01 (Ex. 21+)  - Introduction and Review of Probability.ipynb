{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.21**.\n",
    "\n",
    "**(a)**  Show that, for uncorrelated random variables, the expected value of the product is equal to the product of the expected values (by definition, $X$ and $Y$ are uncorrelated if $\\text{E}[(X - \\overline{X})(Y - \\overline{Y})] = 0$).\n",
    "\n",
    "**(b)**  Show that if $X$ and $Y$ are uncorrelated, then the variance of $X + Y$ is equal to the variance of $X$ plus the variance of $Y$.\n",
    "\n",
    "**(c)**  Show that if $X_1, \\dots, X_n$ are uncorrelated, then the variance of the sum is equal to the sum of variances.\n",
    "\n",
    "**(d)**  Show that independent random variables are uncorrelated.\n",
    "\n",
    "**(e)**  Let $X, Y$ be identically distributed ternary valued random variables with the PMF $p_X(-1) = p_X(1) = 1/4$, $p_X(0) = 1/2$.  Find a simple joint probability assignment such that $X$ and $Y$ are uncorrelated but dependent.\n",
    "\n",
    "**(f)**  You have seen that the MGF of a sum of independent random variables is equal to the product of the individual MGFs.  Give an example where this is false if the variables are uncorrelated but dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{E}[(X - \\overline{X})(Y - \\overline{Y})] &= \\text{E}[XY - \\overline{X}Y - X\\overline{Y} + \\overline{X}\\overline{Y}] \\\\\n",
    "&= \\text{E}[XY] - \\overline{Y}\\text{E}[X] - \\overline{X}\\text{E}[Y] + \\overline{X}\\overline{Y} \\\\\n",
    "&= \\text{E}[X]\\text{E}[Y] - \\overline{Y}\\text{E}[X] - \\overline{X}\\text{E}[Y] + \\overline{X}\\overline{Y} \\\\\n",
    "&= (\\text{E}[X] - \\overline{X})(\\text{E}[Y] - \\overline{Y}) \\\\\n",
    "&= 0 \\cdot 0 = 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Var}[X + Y] &= \\text{E}[(X+Y)^2] - \\text{E}[X + Y]^2  \\\\\n",
    "&= \\text{E}[X^2 + 2XY + Y^2] - (\\text{E}[X] + \\text{E}[Y])^2 \\\\\n",
    "&= \\text{E}[X^2] + 2 \\text{E}[XY] + \\text{E}[Y^2] - (\\text{E}[X]^2 + 2 \\text{E}[X] \\text{E}[Y] + \\text{E}[Y]^2) \\\\\n",
    "&= \\text{E}[X^2] - \\text{E}[X]^2 + \\text{E}[Y^2] - \\text{E}[Y]^2 \\\\\n",
    "&= \\text{Var}[X] + \\text{Var}[Y]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  The result follows from (b), by induction on the number of variables, and making $X = X_1 + \\dots + X_{n-1}$, $Y = X_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** If $X$, $Y$ are independent,\n",
    "\n",
    "$$ F_{X, Y}(x, y) = F_X(x) F_Y(y) $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\text{E}[XY] = \\int_{X, Y} xy dF_{X, Y}(x, y) = \\int_X \\int_Y x y dF_X(x) dF_Y(y) = \\int_X x dF_X(x) \\int_Y y dF_Y(y) = \\text{E}[X] \\text{E}[Y] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**  Consider the joint distribution:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|ccc|c}\n",
    " & Y = -1 & Y = 0 & Y = 1 & \\\\\n",
    "\\hline\n",
    "X = -1 & 1/8 & 0   & 1/8 & 1/4 \\\\\n",
    "X = 0  & 0   & 1/2 & 0   & 1/2 \\\\\n",
    "X = 1  & 1/8 & 0   & 1/8 & 1/4 \\\\\n",
    "\\hline\n",
    "       & 1/4 & 1/2 & 1/4 & 1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We have:\n",
    "\n",
    "- $\\text{E}[XY] = \\text{E}[X] = \\text{E}[Y] = 0$, so $\\text{E}[XY] = \\text{E}[X]\\text{E}[Y]$, and $X, Y$ are uncorrelated.\n",
    "- $\\text{Pr}\\{X = 0 | Y = 0\\} = 1 \\neq \\text{Pr}\\{X = 0\\} = 1/2$, so $X, Y$ are dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)**  Consider the variables $X, Y$ defined above on the solution to (e).  Then the MGFs for $X, Y$ are:\n",
    "\n",
    "$$ g_X(r) = \\text{E}[e^{rX}] = \\frac{1}{4}(e^{-r} + e^r + 2)\n",
    "\\quad \\text{and} \\quad\n",
    "g_Y(r) = \\text{E}[e^{rY}] = \\frac{1}{4}(e^{-r} + e^r + 2)$$\n",
    "\n",
    "but the MGF for $X + Y$ (which only assumes values -2, 0, 2) is:\n",
    "\n",
    "$$ g_{X + Y}(r) = \\text{E}[e^{r(X + Y)}] = \\frac{1}{8}(e^{-2r} + e^{2r} + 6) $$\n",
    "\n",
    "which is distinct from\n",
    "\n",
    "$$ g_X(r) g_Y(r) = \\frac{1}{16}(e^{-r} + e^r + 2)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.22**.  Suppose that $X$ has the Poisson PMF, $p_X(n) = \\lambda^n \\exp (-\\lambda) / n!$ for $n \\geq 0$  and $Y$ has the Poisson PMF, $p_Y(n) = \\mu^n \\exp (-\\mu) / n!$ for $n \\geq 0$.  Assume that $X$ and $Y$ are independent.  Show that $Z = X + Y$ is a Poisson random variable and find the conditional distribution of $Y$ conditional on $Z = n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  Since $X$ and $Y$ are independent, the MGF of $X + Y$ is the product of the MGFs of $X$ and $Y$,\n",
    "\n",
    "$$ g_Z(r) = g_{X + Y}(r) = g_X(r) g_Y(r) = e^{\\lambda (e^r - 1)} e^{\\mu (e^r - 1)} = e^{(\\lambda + \\mu) (e^r - 1)}$$\n",
    "\n",
    "This means that $Z$ has the same MGF as a Poisson distribution with parameter $\\lambda + \\mu$ -- so $Z \\sim \\text{Poisson}(\\lambda + \\mu)$.\n",
    "\n",
    "The conditional distribution of $Y$ conditional on $Z = n$ has PMF\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Pr}\\{Y = y | X + Y = n\\} &= \\frac{\\text{Pr}\\{Y = y, Z = n\\}}{\\text{Pr}\\{Z = n\\}} = \\frac{\\text{Pr}\\{Y = y, X = n - y\\}}{\\text{Pr}\\{Z = n\\}} = \\frac{\\text{Pr}\\{Y = y\\} \\text{Pr}\\{ X = n - y\\}}{\\text{Pr}\\{Z = n\\}}\\\\\n",
    "&= \\frac{\\mu^y e^{-\\mu}}{y!} \\frac{\\lambda^{n-y} e^{-\\lambda}}{(n - y)!} \\cdot \\frac{n!}{(\\lambda + \\mu)^n e^{-(\\lambda + \\mu)}} \\\\\n",
    "&= \\binom{n}{y} p^y (1 - p)^{n - y}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $p = \\frac{\\mu}{\\lambda + \\mu}$.\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$Y | Z = n \\sim \\text{Binomial}\\left(n, \\frac{\\mu}{\\lambda + \\mu} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.23**.\n",
    "\n",
    "**(a)**  Suppose $X, Y, Z$ are binary random variables, each taking on the value 0 with probability 1/2 and the value 1 with probability 1/2.  Find a simple example in which $X, Y, Z$ are statistically dependent but are pairwise statistically independent (i.e. $X, Y$ are statistically independent, $X, Z$ are statistically independent, and $Y, Z$ are statistically independent).  Give $p_{XYZ}(x, y, z)$ for your example.  Hint:  In the simplest example, there are four joint values for $x, y, z$ that have probability 1/4 each.\n",
    "\n",
    "**(b)**  Is pairwise statistical independence enough to ensure that\n",
    "\n",
    "$$ \\text{E} \\left[ \\prod_{i=1}^n X_i \\right] = \\prod_{i=1}^n \\text{E}[X_i] $$\n",
    "\n",
    "for a set of random variables $X_1, \\dots, X_n$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  Consider the joint distribution defined over sample space $\\Omega = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4 \\}$, where each singleton event $\\{ \\omega_i \\}$ is equiprobable:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|ccc|ccc|c}\n",
    "\\omega & X_1 & X_2 & X_3 & X_1 X_2 & X_1 X_3 & X_2 X_3 & X_1 X_2 X_3 \\\\\n",
    "\\hline\n",
    "\\omega_1 & 1 & 1 & 0 & 1 & 0 & 0 & 0\\\\\n",
    "\\omega_2 & 1 & 0 & 1 & 0 & 1 & 0 & 0\\\\\n",
    "\\omega_3 & 0 & 1 & 1 & 0 & 0 & 1 & 0\\\\\n",
    "\\omega_4 & 0 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We have:\n",
    "\n",
    "- $ \\text{E}[X_i] = 1/2 $ for $i = 1, 2, 3$\n",
    "- $ \\text{E}[X_i X_j] = 1/4 $ for $i, j \\in \\{ (1, 2), (1, 3), (2, 3) \\}$\n",
    "- $ \\text{E}[X_1 X_2 X_3] = 0$\n",
    "\n",
    "Therefore, we have pairwise statistical independence but overall statistical dependence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  No -- the scenario in (a) is a counterexample for 3 variables, and scenarios for arbitrarily more variables can be constructed by adding an arbitrary number of constant random variables $X_k(\\omega) = 1$ for $k > 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.24**.  Show that $\\text{E}[X]$ is the value of $\\alpha$ that minimizes $\\text{E}[(X - \\alpha)^2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have:\n",
    "\n",
    "$$ f(\\alpha) = \\text{E}[(X - \\alpha)^2] = \\text{E}[X^2] - 2\\alpha\\text{E}[X] + \\alpha^2 $$\n",
    "\n",
    "The function $f$ is a second degree polynomial in $\\alpha$ with a leading coefficient of 1.  It has a single minimum, reached at $f'(\\alpha) = 0$, but\n",
    "\n",
    "$$ f'(\\alpha) = 2\\alpha - 2 \\text{E}[X] $$\n",
    "\n",
    "so the minimum is reached at $\\alpha = \\text{E}[X]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.25**.  For each of the following random variables, find the endpoints $r_-$ and $r_+$ of the interval for which the MGF $g(r)$ exists.  Determine in each case whether $g_X(r)$ exists at $r_-$ and $r_+$.  For (a) and (b) you should also find and sketch $g(r)$.  For (c) and (d), $g(r)$ has no closed form.\n",
    "\n",
    "**(a)** Let $\\lambda, \\theta$ be positive numbers and let $X$ have the density\n",
    "\n",
    "$$ \n",
    "f_X(x) = \\begin{cases}\n",
    "\\frac{1}{2}\\lambda \\exp(-\\lambda x) &\\text{if } x \\geq 0 \\\\\n",
    "\\frac{1}{2}\\theta \\exp(\\theta x) &\\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**(b)** Let $Y$ be a Gaussian random variable with mean $m$ and variance $\\sigma^2$.\n",
    "\n",
    "**(c)** Let $Z$ be a non-negative random variable with density\n",
    "\n",
    "$$ f_Z(z) = k(1 + z)^{-2} \\exp (-\\lambda z) \\quad z \\geq 0$$\n",
    "\n",
    "where $\\lambda > 0$ and $k = \\left[ \\int_{z \\geq 0} (1 + z)^{-2} \\exp(-\\lambda z) dz \\right]^{-1}$.  Hint:  Do not try to evaluate $g_Z(r)$.  Instead, investigate values of $r$ for which the integral is finite and infinite.\n",
    "\n",
    "**(d)** For the $Z$ of (c), find the limit of $\\gamma'(r)$ as $r$ approaches from below.  Then replace $(1 + z)^2$ with $|1 + z|^3$  in the definition of $f_Z(z)$ and $k$ and show whether the above limit is then finite or not.  Hint:  use tilted random variables; no integration is required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
