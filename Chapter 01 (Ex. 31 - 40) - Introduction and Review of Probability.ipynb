{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.31 (Alternative approach 2 to the Markov inequality)**.\n",
    "\n",
    "**(a)**  Minimize $\\text{E}[Y]$ over all non-negative random variables such that $\\text{Pr}\\{Y \\geq b\\} \\geq \\beta$ for some given $b > 0$ and $0 < \\beta < 1$.  Hint:  Use a graphical argument similar to that in Figure 1.7.  What is the random variable that achieves this minimum.  Hint:  It is binary.\n",
    "\n",
    "**(b)**  Use (a) to prove the Markov inequality and also point out the distribution that meets the inequality with equality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The graphical argument is that $\\text{Pr}\\{Y \\geq b \\} = F^c_Y(b)$, the complement of the CDF computed at point $b$.  The inequality requires that this curve assume the value of at least $\\beta$ when evaluated at point $b$, and since $F_Y^c$ is non-increasing, $F_Y^c(y) \\geq \\beta$ for $y \\leq b$.  But $\\text{E}[Y] = \\int F_Y^c(y) dy$, so \n",
    "\n",
    "$$ \\text{E}[Y] \\int_0^\\infty F_Y^c(y) dy \\geq \\int_0^b \\beta dy = y \\beta $$\n",
    "\n",
    "In particular, $Y \\sim b \\cdot \\text{Binary}(\\beta)$ achieves this minimum, as $\\text{Pr}\\{Y \\geq b \\} = \\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** By choosing an arbitrary $b$ and then picking $\\beta = \\text{Pr}\\{Y \\geq b \\}$, from (a) we get\n",
    "\n",
    "$$ \\text{E}[Y] \\geq b \\cdot \\text{Pr}\\{Y \\geq b\\} \\Longrightarrow \\text{Pr}\\{Y \\geq b\\} \\leq \\frac{\\text{E}[Y]}{b} $$\n",
    "\n",
    "Once again, for the distribution $Y \\sim b \\cdot \\text{Binary}(\\beta)$ the equality case holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.32 (The one-sided Chebyshev inequality)**.  This inequality states that if a zero-mean random variable $X$ has variance $\\sigma^2$, then it satisfies the inequality\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2} \\quad \\text{for every } b > 0 \\tag{1.101} $$\n",
    "\n",
    "with equality for some $b$ only if $X$ is binary and $\\text{Pr}\\{X = b\\} = \\sigma^2 / (\\sigma^2 + b^2)$.  We prove this here using the same approach as in Exercise 1.31.  Let $X$ be a zero-mean random variable that satisfies $\\text{Pr}\\{X \\geq b\\} = \\beta$ for some $b > 0$ and $0 < \\beta < 1$.  The variance $\\sigma^2$ of $X$ can be expressed as\n",
    "\n",
    "$$ \\sigma^2 = \\int_{-\\infty}^{b^-} x^2 f_X(x) dx + \\int_b^{\\infty} x^2 f_X(x) dx \\tag{1.102}$$\n",
    "\n",
    "We will first minimize $\\sigma^2$ over all zero-mean $X$ satisfying $\\text{Pr}\\{X \\geq b\\} = \\beta$.\n",
    "\n",
    "**(a)**  Show that the second integral in (1.102) satisfies $\\int_b^\\infty x^2 f_X(x) dx \\geq b^2 \\beta$.\n",
    "\n",
    "**(b)**  Show that the first integral in (1.102) is constrained by \n",
    "\n",
    "$$ \n",
    "\\int_{-\\infty}^{b^-} f_X(x) dx = 1 - \\beta \n",
    "\\quad \\text{and} \\quad \n",
    "\\int_{-\\infty}^{b^-} x f_X(x) dx \\leq -b \\beta\n",
    "$$\n",
    "\n",
    "**(c)**  Minimize the first integral in (1.102) subject to the constraints in (b).  Hint: If you scale $f_X(x)$ up by $1 / (1 - \\beta)$, it integrates to 1 over $(-\\infty, b)$ and the second constraint becomes an expectation.  You can then minimize the first integral in (1.102) by inspection.\n",
    "\n",
    "**(d)**  Combine the results in (a) and (c) to show that $\\sigma^2 \\geq b^2 \\beta / (1 - \\beta)$.  Find the minimizing distribution.  Hint:  It is binary.\n",
    "\n",
    "**(e)**  Use (d) to establish (1.101).  Also show (trivially) that if $Y$ has a mean $\\overline{Y}$ and variance $\\sigma^2$, then $\\text{Pr}\\{Y - \\overline{Y} \\geq b\\} \\leq \\sigma^2 / (\\sigma^2 + b^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "$$ \\int_b^\\infty x^2 f_X(x) dx \\geq \\int_b^\\infty b^2 f_X(x) dx = b^2 \\left( \\int_b^\\infty f_X(x) dx \\right) = b^2 \\text{Pr}\\{ X \\geq b \\} = b^2 \\beta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "We have:\n",
    "\n",
    "$$ \\int_{-\\infty}^{b^-} f_X(x) dx = \\text{Pr}\\{X < b\\} = 1 - \\text{Pr}\\{ X \\geq b \\} = 1 - \\beta $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\int_{-\\infty}^{b^-} x f_X(x) dx &= \\text{E}[X] - \\int_b^\\infty x f_X(x) dx = 0 - \\int_b^\\infty x f_X(x) dx \\\\\n",
    "&\\leq - \\int_b^\\infty b f_X(x) dx = -b \\int_b^\\infty f_X(x) dx \\\\\n",
    "&= -b \\; \\text{Pr}\\{ X \\geq b \\} = -b \\beta\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Multiplying the first constraint in (b) by $1 / (1 - \\beta)$, we get\n",
    "\n",
    "$$ \\int_{-\\infty}^{b^-} \\frac{f_X(x)}{1 - \\beta} dx = \\frac{1 - \\beta}{1 - \\beta} = 1 $$\n",
    "\n",
    "This suggests we can define another probability density,\n",
    "\n",
    "$$ f_Y(y) = \\begin{cases}\n",
    "\\frac{f_X(y)}{1 - \\beta} &\\text{if } y < b \\\\\n",
    "0 &\\text{otherwise }\n",
    "\\end{cases} $$\n",
    "\n",
    "for which we can define the expectation \n",
    "\n",
    "$$ \\text{E}_Y[X] = \\int_{-\\infty}^\\infty x f_Y(x) dx = \\frac{1}{1 - \\beta} \\int_{-\\infty}^{b^-} x f_X(x) dx$$\n",
    "\n",
    "and so the second constraint can be expressed as\n",
    "\n",
    "$$ \\int_{-\\infty}^{b^-} x f_X(x) dx = (1 - \\beta) \\int_{-\\infty}^\\infty x f_Y(x) dx = (1 - \\beta)  \\text{E}_Y[X] \\leq -b \\beta$$\n",
    "\n",
    "or, with a bound on the expectation of $X$ under $Y$,\n",
    "\n",
    "$$  \\text{E}_Y[X] \\leq -b \\frac{\\beta}{1 - \\beta} $$\n",
    "\n",
    "Therefore, the expectation of $X^2$ under $Y$ is\n",
    "\n",
    "$$ \\text{E}_Y[X^2] = \\text{Var}_Y[X] + \\text{E}_Y[X]^2 \\geq \\text{E}_Y[X]^2 \\geq b^2 \\frac{\\beta^2}{(1 - \\beta)^2} $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\int_{-\\infty}^{b^-} x^2 f_X(x) dx = (1 - \\beta) \\text{E}_Y[X^2] \\geq b^2 \\frac{\\beta^2}{1 - \\beta} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  We have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma^2 &= \\int_{-\\infty}^{b^-} x^2 f_X(x) dx + \\int_b^{\\infty} x^2 f_X(x) dx \\\\\n",
    "&\\geq b^2 \\frac{\\beta^2}{1 - \\beta} + b^2 \\beta \\\\\n",
    "&= b^2 \\frac{\\beta}{1 - \\beta}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To find an example where the equality holds, consider a Bernoulli (binary) variable with parameter $p$, shifted to have mean 0:\n",
    "\n",
    "$$ X \\sim \\text{Bernoulli}(p) - p $$\n",
    "\n",
    "By construction, $\\text{E}[X] = 0$, and $\\sigma^2 = \\text{Var}[X] = p(1 - p)$.  Then, taking $b = 1 - p$,\n",
    "\n",
    "$$ \\beta = \\text{Pr}\\{X \\geq 1 - p\\} = p \\quad \\text{and} \\quad b^2 \\frac{\\beta}{1 - \\beta} = (1-p)^2 \\frac{p}{1 - p} = p(1 - p) = \\text{Var}[X] $$ \n",
    "\n",
    "The one-sided Chebyshev inequality also provides an equality bound at this $b$:\n",
    "\n",
    "$$ \\frac{\\sigma^2}{\\sigma^2 + b^2} = \\frac{p(1-p)}{p(1-p) + (1-p)^2} = \\frac{p}{p + (1 - p)} = p $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**  From (d),\n",
    "\n",
    "$$ \\sigma^2 \\geq b^2 \\frac{\\beta}{1 - \\beta} $$\n",
    "\n",
    "and since $g(x) = \\frac{x}{x + b^2}$ is increasing in $x$, $x > 0$,\n",
    "\n",
    "$$ \\frac{\\sigma^2}{\\sigma^2 + b^2} \\geq \\frac{b^2\\frac{\\beta}{1 - \\beta}}{b^2\\frac{\\beta}{1 - \\beta} + b^2} = \\frac{\\beta}{\\beta + (1 - \\beta)} = \\beta = \\text{Pr}\\{X \\geq b \\}$$\n",
    "\n",
    "which is our result.\n",
    "\n",
    "Also, trivially, if $Y$ has mean $\\overline{Y}$, we can define the random variable $X = Y - \\overline{Y}$ with $\\text{Var}(X) = \\text{Var}(Y) = \\sigma^2$, so $\\text{Pr}\\{X \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2}$ implies $\\text{Pr}\\{Y - \\overline{Y} \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.33 (Proof of (1.48))**.  Here we show that if $X$ is a zero-mean random variable with a variance $\\sigma^2$, then the median $\\alpha$ satisfies $|\\alpha| \\leq \\sigma$.\n",
    "\n",
    "**(a)**  First show that $|\\alpha| \\leq \\sigma$ for the special case where $X$ is binary with equiprobable values at $\\pm \\sigma$.\n",
    "\n",
    "**(b)** For all zero-mean random variables $X$ with variance $\\sigma^2$ other than the special case in (a), show that\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\sigma\\} < 0.5 $$\n",
    "\n",
    "Hint:  Use the one-sided Chebyshev inequality of Exercise 1.32.\n",
    "\n",
    "**(c)** Show that $\\text{Pr}\\{X \\geq \\alpha\\} \\geq 0.5$.  Other than the special case in (a), show that this implies that $\\alpha < \\sigma$.\n",
    "\n",
    "**(d)** Other than the special case in (a), show that $|\\alpha| < \\sigma$.  Hint:  Repeat (b) and (c) for the random variable $-X$.  You have then shown that $|\\alpha| \\leq \\sigma$ with equality only for the binary case with values $\\pm \\sigma$.  For random variables $Y$ with a non-zero mean, this shows that $|\\alpha - \\overline{Y}| \\leq \\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  For this particular distribution, the interval of medians is $(-\\sigma, \\sigma)$, since for any $\\alpha$ in this interval $\\text{Pr}\\{ X \\leq \\alpha \\} = \\text{Pr}\\{ X \\geq \\alpha \\} = 0.5$ (counting only the probability mass at $-\\sigma$).  Then $\\alpha \\in (-\\sigma, \\sigma)$ implies $|\\alpha| \\leq \\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Applying the one-sided Chebyshev inequality from Exercise 1.32,\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2} \\quad \\text{for every } b > 0 $$\n",
    "\n",
    "Choose $b = \\sigma$, and we get\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\sigma \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + \\sigma^2} = 0.5 $$\n",
    "\n",
    "We can also demonstrate the one-sided Chebyshev inequality directly from Markov's inequality; for any $u \\geq 0$,\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq b\\} \\leq \\text{Pr}\\{(X + u)^2 \\geq (b + u)^2\\} \\leq \\frac{\\text{E}[(X + u)^2]}{(b + u)^2} = \\frac{\\sigma^2 + u^2}{(b + u)^2}$$\n",
    "\n",
    "and the bound is minimized by $u = \\sigma^2 / b$, leading to \n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2} $$\n",
    "\n",
    "Therefore, equality in the one-sided Chebyshev occurs if and only if we have equality in the Markov distribution above, which implies the distribution given in (a).  So, for all other cases,\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\sigma \\} < 0.5 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  By definition, any median $\\alpha$ satisfies $\\text{Pr}\\{X \\geq \\alpha\\} \\geq 0.5$, and so\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\alpha\\} \\geq 0.5 \\geq \\text{Pr}\\{X \\geq \\sigma \\} $$\n",
    "\n",
    "For distributions other than the distribution in (a), the second inequality is not an equality, and so\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\alpha\\} > \\text{Pr}\\{X \\geq \\sigma \\} $$\n",
    "\n",
    "Since the complementary CDF $F_X^c(x) = \\text{Pr}\\{ X \\geq x \\}$ is non-increasing, this implies that $\\alpha < \\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Repeating the argument of (b) and (c) for the random variable $-X$ following a distribution other than the one in (a), its medians $\\alpha'$ satisfy $\\alpha' < \\sigma$. But if $\\alpha'$ is a median of $-X$, then $-\\alpha'$ is a median of $X$, so this implies that $\\alpha > -\\sigma$.  Therefore, we have $-\\sigma < \\alpha < \\sigma$, or $|\\alpha| < \\sigma$.  (For the distribution in (a), the same argument applies with equalities instead.)\n",
    "\n",
    "It does follow that we can translate any arbitrary variable $Y$ to a zero-mean variable $X$ doing $X = Y - \\overline{Y}$; the medians also get translated by $-\\overline{Y}$, and thus we have shown that $|\\alpha - \\overline{Y}| \\leq \\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.34**.  We stressed the importance of the mean of a random variable $X$ in terms of association with its sample average via the WLLN.  Here we show that in essence the WLLN allows us to evaluate the entire CDF, say $F_X(x)$ of $X$, via sufficiently many independent sample values of $X$.\n",
    "\n",
    "**(a)** For any given $y$, let $\\mathbb{I}_j(y)$ be the indicator function of the event $\\{X_j \\leq y\\}$, where $X_1, X_2, \\dots, X_j, \\dots$ are IID random variables with the CDF $F_X(x)$.  State the WLLN for the IID random variables $\\{ \\mathbb{I}_1(y), \\mathbb{I}_2(y), \\dots\\}$.\n",
    "\n",
    "**(b)** Does the answer to (a) require $X$ to have a mean or variance?\n",
    "\n",
    "**(c)** Suggest a procedure for evaluating the median of $X$ from the sample values of $X_1, X_2, \\dots$.  Assume that $X$ is a continuous random variable and that its PDF is positive in an open interval around the median.  You need not be precise, but try to think the issue through carefully.\n",
    "\n",
    "What you have seen here, without stating it precisely or proving it is that the median has a law or large numbers associated with it, saying that the sample median of $n$ IID samples of a random variable is close to the true median with high probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  Consider the sample averages of the indicator sequence,\n",
    "\n",
    "$$ S_{\\mathbb{I}_n(y)} = \\frac{1}{n} \\sum_{j=1}^n \\mathbb{I}_j(y) $$\n",
    "\n",
    "Its expected value is \n",
    "\n",
    "$$ \\text{E}[S_{\\mathbb{I}_n(y)}] = \\text{E}[\\mathbb{I}_1(y)] = \\text{Pr}\\{X_1 \\leq y\\} = F_X(y) $$\n",
    "\n",
    "and so the WLLN states that\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr} \\left\\{ \\left| \\frac{S_{\\mathbb{I}_n(y)} }{n} - F_X(y) \\right| > \\epsilon \\right\\} = 0 \\quad \\text{for every } \\epsilon > 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  No.  The indicator variable sequence is required to have a mean, but that follows from the sequence approaching the CDF of $X$ at a given point $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  For any given $y$, we can approximate $F_X(y)$ by computing the sample average of the sequence $\\{\\mathbb{I}_1(y), \\mathbb{I}_2(y), \\dots \\}$.  For the (unknown) median $\\tilde{\\mu}$, we have $F_X(\\tilde{\\mu}) = 1/2$, so half of the terms of the original sequence $X_1, X_2, \\dots$, would be under $\\tilde{\\mu}$ and half over $\\tilde{\\mu}$:\n",
    "\n",
    "$$ \\text{Pr}\\{ X_i \\leq \\tilde{\\mu} \\} = \\text{Pr}\\{ X_i \\geq \\tilde{\\mu} \\} = 1/2 $$\n",
    "\n",
    "Therefore, if we get the sample median of the sequence of $X_i$'s $\\overline{\\mu}_n$, the weak law of large numbers state that\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr} \\left\\{ \\left| \\frac{S_{\\mathbb{I}_n(\\overline{\\mu}_n)} }{n} - F_X(\\overline{\\mu}_n) \\right| > \\epsilon \\right\\} = 0 $$\n",
    "\n",
    "But the sample average of the indicator sequence for the sample median converges to 1/2 by construction -- we defined $\\overline{\\mu}_n$ as the median of $\\{X_1, \\dots, X_n\\}$ -- so\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr} \\left\\{ \\left| \\frac{S_{\\mathbb{I}_n(\\overline{\\mu}_n)} }{n} - \\frac{1}{2} \\right| > \\epsilon \\right\\} = 0 $$\n",
    " \n",
    "which implies $ \\lim_{n \\rightarrow \\infty} F_X(\\overline{\\mu}_n) = 1/2$ and so $ \\lim_{n \\rightarrow \\infty} \\overline{\\mu}_n = \\tilde{\\mu} $, that is, the sample median converges to the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.35**. \n",
    "\n",
    "**(a)** Show that for any $0 < k < n$\n",
    "\n",
    "$$ \\binom{n}{k+1} \\leq \\binom{n}{k} \\frac{n - k}{k} $$\n",
    "\n",
    "**(b)** Extend (a) to show that, for all $\\ell \\leq n - k$,\n",
    "\n",
    "$$ \\binom{n}{k + \\ell} \\leq \\binom{n}{k} \\left[ \\frac{n - k}{k} \\right]^\\ell $$\n",
    "\n",
    "**(c)** Let $\\tilde{p} = k / n$ and $\\tilde{q} = 1 - \\tilde{p}$.  Let $S_n$ be the sum of $n$ binary IID random variables with $p_X(0) = q$ and $p_X(1) = p$.  Show that for all $\\ell \\leq n - k$,\n",
    "\n",
    "$$ p_{S_n}(k + \\ell) \\leq p_{S_n}(k) \\left( \\frac{\\tilde{q} p}{\\tilde{p} q} \\right)^\\ell $$\n",
    "\n",
    "**(d)** For $k / n > p$, show that\n",
    "\n",
    "$$ \\text{Pr}\\{ S_n \\geq pn \\} \\leq \\frac{\\tilde{p} q}{\\tilde{p} - p} p_{S_n}(k) $$\n",
    "\n",
    "**(e)** Now let $\\ell$ be fixed and $k = \\lceil n \\tilde{p} \\rceil$ for fixed $\\tilde{p}$ such that $1 > \\tilde{p} > p$.  Argue that as $n \\rightarrow \\infty$,\n",
    "\n",
    "$$\n",
    "p_{S_n}(k + \\ell) \\sim p_{S_n}(k) \\left( \\frac{\\tilde{q} p}{\\tilde{p} q} \\right)^\\ell\n",
    "\\quad \\text{and} \\quad\n",
    "\\text{Pr}\\{S_n \\geq pn \\} \\sim \\frac{\\tilde{p} q}{\\tilde{p} - p} p_{S_n}(k)\n",
    "$$\n",
    "\n",
    "where $a(n) \\sim b(n)$ means that $\\lim_{n \\rightarrow \\infty} a(n) / b(n) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "$$ \\binom{n}{k+1} = \\frac{n!}{(k + 1)! (n - k - 1)!} = \\frac{n!}{k!(n - k)!} \\frac{n - k}{k+1} = \\binom{n}{k} \\frac{n - k}{k+1} < \\binom{n}{k} \\frac{n - k}{k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  For $\\ell \\geq 1$, from (a),\n",
    "\n",
    "$$ \\binom{n}{k + \\ell} \\leq \\binom{n}{k + \\ell - 1} \\frac{n - k}{k + \\ell - 1} \\leq \\binom{n}{k + \\ell - 1} \\frac{n - k}{k} $$\n",
    "\n",
    "Applying this result $\\ell$ times we get\n",
    "\n",
    "$$ \\binom{n}{k + \\ell} \\leq \\binom{n}{k + \\ell - 1} \\frac{n - k}{k} \\leq \\binom{n}{k + \\ell - 2} \\left[\\frac{n - k}{k}\\right]^2 \\leq \\cdots \\leq \\binom{n}{k} \\left[\\frac{n - k}{k}\\right]^\\ell $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Using the fact that the sum of $n$ IID variables is binomial,\n",
    "\n",
    "$$ p_{S_n}(k + \\ell) = \\binom{n}{k + \\ell} p^{k + \\ell} q^{n - k - \\ell} \n",
    "\\quad \\text{and} \\quad\n",
    "p_{S_n}(k) = \\binom{n}{k} p^k q^{n - k} \n",
    "$$\n",
    "\n",
    "Replacing it into the statement we want to prove,\n",
    "\n",
    "$$ \n",
    "\\binom{n}{k + \\ell} p^{k + \\ell} q^{n - k - \\ell} \\leq \\binom{n}{k} p^k q^{n - k} \\left( \\frac{\\tilde{q} p}{\\tilde{p} q} \\right)^\\ell \n",
    "$$\n",
    "\n",
    "or, isolating $\\binom{n}{k + \\ell}$,\n",
    "\n",
    "$$\n",
    "\\binom{n}{k + \\ell} \\leq \\binom{n}{k} \\left( \\frac{\\tilde{q}}{\\tilde{p}} \\right)^\\ell = \\binom{n}{k} \\left[ \\frac{n - k}{k}\\right]^\\ell\n",
    "$$\n",
    "\n",
    "which is the result from (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  This result follows by adding the inequalities in (c) for $\\ell = 0, \\dots, n - k$ -- the left hand side is the complementary CDF of the binomial distribution, obtained by adding all the probability mass for possible values of $S_n$, while the right hand side is a geometric sum times a probability mass function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**  These bounds are tight, as all inequalities are built from the inequalities in (b), which replace $c(k) = (k +1)(k +2) \\cdots (k + \\ell) $ with $d(k) = k^\\ell$.  For a fixed $\\ell$, $c(n) \\sim d(n)$, and the same applies to all inequalities deduced from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.36**.  A sequence $\\{ a_n : n \\geq 1 \\}$ of real numbers has limit 0 if, for all $\\epsilon > 0$, there is an $m(\\epsilon)$ such that $| a_n | \\leq \\epsilon$ for all $n \\geq m(\\epsilon)$.  Show that the sequences in (a) and (b) below satisfy $\\lim_{n \\rightarrow \\infty} a_n = 0$ but the sequence in (c) does not have a limit.\n",
    "\n",
    "**(a)** $ a_n = \\frac{1}{\\ln (\\ln( n + 1))} $\n",
    "\n",
    "**(b)** $ a_n = n^{10} e^{-n} $\n",
    "\n",
    "**(c)** $a_n = 1$ for $n = 10^\\ell$ for each positive integer $\\ell$ and $a_n = 0$ otherwise.\n",
    "\n",
    "**(d)** Show that the definition can be changed (with no change in meaning) by replacing $\\epsilon$ with either $1 / k$ or $2^{-k}$ for every positive integer $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  \n",
    "\n",
    "$$ \n",
    "a_n \\leq \\epsilon \n",
    "\\Longleftrightarrow \\ln (\\ln (n + 1)) > \\frac{1}{\\epsilon} \n",
    "\\Longleftrightarrow \\ln (n + 1) > \\exp\\left\\{\\frac{1}{\\epsilon}\\right\\}\n",
    "\\Longleftrightarrow n > \\exp\\left\\{\\exp\\left\\{\\frac{1}{\\epsilon}\\right\\}\\right\\} - 1\n",
    "$$\n",
    "\n",
    "So, for each $\\epsilon$, we can pick $m(\\epsilon) = \\exp\\left\\{\\exp\\left\\{\\frac{1}{\\epsilon}\\right\\}\\right\\} - 1$ to satisfy $\\forall n > m(\\epsilon), |a_n| \\leq \\epsilon $.\n",
    "\n",
    "**(b)**  Note that $n^{10} < e^{n/2}$ for sufficiently large $n > n_0$, since the left hand side grows as a polynomial and the right hand side grows exponentially.  Then:\n",
    "\n",
    "$$ a_n \\leq \\epsilon \\Longleftrightarrow n^{10}e^{-n} \\leq \\epsilon \\Longleftarrow e^{n/2}e^{-n} \\leq \\epsilon, n \\geq n_0 \\Longleftrightarrow -\\frac{n}{2} \\leq \\ln \\epsilon, n \\geq n_0 \\Longleftrightarrow n \\geq \\max\\{ -2 \\ln \\epsilon, n_0 \\} $$\n",
    "\n",
    "So, for each $\\epsilon$, we can pick $m(\\epsilon) = \\max\\{ -2 \\ln \\epsilon, n_0 \\}$ to satisfy $\\forall n > m(\\epsilon), |a_n| \\leq \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Assume that the limit exists and is $L$, and that such $m(\\epsilon)$ exist for a given $\\epsilon = 0.5$.  This would imply that $|a_n - L| \\leq 0.5$ for all $n > m(\\epsilon).$  But if $n_0$ is the next power of 10 greater than $m(\\epsilon)$, then $a_{n_0} = 1$ and $a_{n_0 + 1} = 0$, so we must have $|1 - L| < 0.5$ and $|L| < 0.5$, and there is no $L$ that can satisfy both statements, a contradiction.  Therefore the sequence does not have a limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** The definition can be changed by replacing arbitrary real $\\epsilon$ with $1/k$ or $2^{-k}$.  \n",
    "\n",
    "In one direction, if the limit condition holds for all $\\epsilon > 0$, it will also hold for all $1 / k$ and for all $2^{-k}$, since $\\{ x : x = 1 / k, k > 0, k \\in \\mathbb{Z} \\}$ and $\\{ x : x = 2^{-k}, k > 0, k \\in \\mathbb{Z} \\}$ are both subsets of $\\{ \\epsilon : \\epsilon > 0 \\}$.\n",
    "\n",
    "In the other direction, if the condition holds for $1 / k$ for a particular $k$, it must also hold for any $\\epsilon < 1/k$.  Therefore, for any given $\\epsilon$, we can find integer $k > 0$ such that $\\epsilon < 1/k$, and so the condition must hold for $\\epsilon$.  A similar argument applies to showing that a definition with $2^{k}$ implies a definition with $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.37**.  Represent the MGF of a random variable $X$ by\n",
    "\n",
    "$$ g_X(r) = \\int_{-\\infty}^0 e^{rx} dF(x) + \\int_0^\\infty e^{rx} dF(x) $$\n",
    "\n",
    "In each of the following parts, you are welcome to restrict $X$ to be either discrete or continuous.\n",
    "\n",
    "**(a)**  Show that the first integral always exists (i.e. is finite) for $r \\geq 0$ and that the second integral always exists for $r \\leq 0$.\n",
    "\n",
    "**(b)**  Show that if the second integral exists for a given $r_1 > 0$, then it also exists for all $r$ in the range $0 \\leq r \\leq r_1$.\n",
    "\n",
    "**(c)**  Show that if the first integral exists for a given $r_2 < 0$, then it also exists for all $r$ in the range $r_2 \\leq r \\leq 0$.\n",
    "\n",
    "**(d)**  Show that the range of $r$ over which $g_X(r)$ exists is an interval from some $r_- \\leq 0$ to some $r_+ \\geq 0$ (the interval might or might not include each endpoint, and the magnitude of either or both endpoints might be 0, $\\infty$, or any point between).\n",
    "\n",
    "**(e)**  Find an example where $r_+ = 1$ and the MGF does not exist for $r = 1$.  Find another example where $r_+ = 1$ and the MGF does exist for $r = 1$.  Hint:  Consider $f_X(x) = e^{-x}$ for $x \\geq 0$ and figure out how to modify it to $f_Y(y)$ so that $\\int_0^\\infty e^y f_Y(y) < \\infty$ but $\\int_0^\\infty e^{y + \\epsilon y} f_Y(y) dy = \\infty$ for all $\\epsilon > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  For $r \\geq 0$,  $x \\leq 0$, $0 \\geq e^{rx} \\leq e^0 = 1$, so the first integral may be bound as\n",
    "\n",
    "$$ 0 \\leq \\int_{-\\infty}^0 e^{rx} dF(x) \\leq \\int_{-\\infty}^0 1 \\cdot dF(x) \\leq \\int_{-\\infty}^\\infty dF(x) = 1 $$\n",
    "\n",
    "therefore it is finite.\n",
    "\n",
    "Similarly, for $r \\leq 0$, $x \\geq 0$, $0 \\geq e^{rx} \\leq e^0 = 1$, so the second integral may be bound as\n",
    "\n",
    "$$ 0 \\leq \\int_0^\\infty e^{rx} dF(x) \\leq \\int_0^\\infty 1 \\cdot dF(x) \\leq \\int_{-\\infty}^\\infty dF(x) = 1$$\n",
    "\n",
    "therefore it is finite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  If the second integral exists for a given $r_1 > 0$, then for $x \\geq 0$, $0 \\leq r \\leq r_1$, $0 < e^{rx} \\leq e^{r_1x}$, so\n",
    "\n",
    "$$ 0 < \\int_0^\\infty e^{rx} dF(x) \\leq \\int_0^\\infty e^{r_1x} dF(x) < \\infty $$\n",
    "\n",
    "so the second integral exists for $0 \\leq r \\leq r_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  If the second integral exists for a given $r_2 < 0$, then for $x \\leq 0$, $r_2 \\leq r \\leq 0$, $0 < e^{rx} < e^{r_2x}$, so\n",
    "\n",
    "$$ 0 \\leq \\int_{-\\infty}^0 e^{rx} dF(x) \\leq \\int_{-\\infty}^0 e^{r_2 x} dF(x) < \\infty $$\n",
    "\n",
    "so the second integral exists for $r_2 \\leq r \\leq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  (a) implies that the first integral exists for any $r \\geq 0$, and the second integral exists for any $r \\leq 0$.  Along with (b), this implies that:\n",
    "\n",
    "- If both integrals exist for some $r_+ > 0$, they must exist for every $r$ between it and 0, $0 \\leq r \\leq r_+$.\n",
    "- If both integrals exist for some $r_- < 0$, they must exist for every $r$ between 0 and it, $r_- \\leq r \\leq 0$.\n",
    "\n",
    "Both integrals will always exist for $r = 0$, since the integrals just add up to integrating over the whole probability density, $\\int dF(x) = 1$.\n",
    "\n",
    "These conditions, together, mean that the set of values where the integral exists must be an interval (open or closed) including 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**  A scenario where $r_+ = 1$ and the MGF does not exist for $r_+$ is the exponential distribution with $\\lambda = 1$, $f_X(x) = e^{-x}$ for $x > 0$.  The MGF is\n",
    "\n",
    "$$ g_X(r) = \\text{E}[e^{rX}] = \\int_0^\\infty e^{rx} f_X(x) dx = \\int_0^\\infty e^{(r - 1)x} dx = \\frac{1}{1 - r}$$\n",
    "\n",
    "for $r < 1$, while if $r = 1$ the integral diverges.\n",
    "\n",
    "Taking inspiration from Exercise 1.25, we can modify the density function by making\n",
    "\n",
    "$$ h_Y(y) = \\frac{e^{-y}}{(1 + y)^2}, \\quad y \\geq 0 $$\n",
    "\n",
    "and normalizing it, so that the density function becomes\n",
    "\n",
    "$$ f_Y(y) = \\frac{h_Y(y)}{\\int_0^\\infty h_Y(t) dt} = \\frac{1}{k} h_Y(y), \\quad y \\geq 0 $$\n",
    "\n",
    "where $k = \\int_0^\\infty h_Y(t) dt$.  Now, the MGF becomes\n",
    "\n",
    "$$ g_Y(r) = \\text{E}[e^{rY}] = \\int_0^\\infty e^{ry} f_Y(y) dy = \\frac{1}{k} \\int_0^\\infty \\frac{e^{(r - 1)y}}{(1 + y)^2} dy$$\n",
    "\n",
    "For $r = 1$, this integral becomes\n",
    "\n",
    "$$ g_Y(1) = \\frac{1}{k} \\int_0^\\infty \\frac{1}{(1 + y)^2} dy = \\frac{1}{k} $$\n",
    "\n",
    "which is finite and defined.  On the other hand, for $r > 1$, the integrand $\\frac{e^{(r - 1)y}}{(1 + y)^2}$ becomes arbitrarily large with $y$, as the numerator grows exponentially and the denominator grows as $O(y^2)$, so the integral does not converge and the MGF is not defined.\n",
    "\n",
    "Therefore, the provided density function is an example where the MGF is defined for values up to $r_+ = 1$ inclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.38**.  Let $\\{X_n: n \\geq 1\\}$ be a sequence of independent but not identically distributed random variables.  We say that the WLLN holds for this sequence if for all $\\epsilon > 0$\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr} \\left\\{ \\left| \\frac{S_n}{n} - \\frac{\\text{E}[S_n]}{n} \\right| > \\epsilon \\right\\} = 0,\n",
    "\\quad \\text{where } S_n = X_1 + X_2 + \\cdots + X_n \\tag{WLLN} $$\n",
    "\n",
    "**(a)** Show that the WLLN holds if there is some constant $A$ such that $\\sigma_{X_n}^2 \\leq A$ for all $n$.\n",
    "\n",
    "**(b)** Suppose that $\\sigma_{X_n}^2 \\leq A n^{1-\\alpha}$ for some $\\alpha < 1$ and for all $n$.  Show that the WLLN holds in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have:\n",
    "\n",
    "$$ \\text{E}[(S_n - \\text{E}[S_n])^2] = \\text{E}\\left[\\left(\\sum_{i=1}^n X_i - \\text{E}[X_i]\\right)^2\\right] = \\sum_{i=1}^n \\text{E}[(X_i - \\text{E}[X_i])^2] \\leq nA $$\n",
    "\n",
    "where we used independence between the $X_i$'s to cancel out the $\\text{E}[(X_i - \\text{E}[X_i])(X_j - \\text{E}[X_j])]$ terms, $i \\neq j$, and we used $\\text{E}[(X_i - \\text{E}[X_i])^2] \\leq A$ for each $i$.\n",
    "\n",
    "Then, for $\\epsilon > 0$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Pr} \\left\\{ \\left| \\frac{S_n}{n} - \\frac{\\text{E}[S_n]}{n} \\right| > \\epsilon \\right\\} = \\text{Pr}\\{(S_n - \\text{E}[S_n])^2 > (n \\epsilon)^2 \\} \\leq \\frac{\\text{E}[(S_n - \\text{E}[S_n])^2]}{n^2 \\epsilon^2} \\leq \\frac{A}{n\\epsilon^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we used Markov's inequality.  But $\\frac{A}{n\\epsilon^2} \\rightarrow 0$ as $n \\rightarrow \\infty$, so\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr} \\left\\{ \\left| \\frac{S_n}{n} - \\frac{\\text{E}[S_n]}{n} \\right| > \\epsilon \\right\\} = 0 $$\n",
    "\n",
    "that is, the WLLN holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** If the constraint instead is $\\sigma_{X_n}^2 \\leq An^{1 - \\alpha}$, we instead get\n",
    "\n",
    "$$ \\text{E}[(S_n - \\text{E}[S_n])^2] = \\text{E}\\left[\\left(\\sum_{i=1}^n X_i - \\text{E}[X_i]\\right)^2\\right] = \\sum_{i=1}^n \\text{E}[(X_i - \\text{E}[X_i])^2] \\leq n^{2 - \\alpha}A $$\n",
    "\n",
    "where we used independence between the $X_i$'s to cancel out the $\\text{E}[(X_i - \\text{E}[X_i])(X_j - \\text{E}[X_j])]$ terms, $i \\neq j$, and we used $\\text{E}[(X_i - \\text{E}[X_i])^2] \\leq n^{1 - \\alpha}A$ for each $i$.\n",
    "\n",
    "Then, for $\\epsilon > 0$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Pr} \\left\\{ \\left| \\frac{S_n}{n} - \\frac{\\text{E}[S_n]}{n} \\right| > \\epsilon \\right\\} = \\text{Pr}\\{(S_n - \\text{E}[S_n])^2 > (n \\epsilon)^2 \\} \\leq \\frac{\\text{E}[(S_n - \\text{E}[S_n])^2]}{n^2 \\epsilon^2} \\leq \\frac{A}{n^\\alpha \\epsilon^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we used Markov's inequality.  But $\\frac{A}{n^\\alpha \\epsilon^2} \\rightarrow 0$ as $n \\rightarrow \\infty$, so\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr} \\left\\{ \\left| \\frac{S_n}{n} - \\frac{\\text{E}[S_n]}{n} \\right| > \\epsilon \\right\\} = 0 $$\n",
    "\n",
    "that is, the WLLN holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.39**.  Let $\\{X_i : i \\geq 1\\}$ be IID binary random variables .  Let $\\text{Pr}\\{X_i = 1\\} = \\delta$, $\\text{Pr}\\{X_i = 0\\} = 1 - \\delta$.  Let $S_n = X_1 + \\cdots + X_n$.  Let $m$ be an arbitrary but fixed positive integer.  Think!  Then evaluate the following and explain your answers:\n",
    "\n",
    "**(a)** $\\lim_{n\\rightarrow \\infty} \\sum_{i: n\\delta - m \\leq i \\leq n\\delta + m} \\text{Pr}\\{S_n = i\\} $\n",
    "\n",
    "**(b)** $\\lim_{n\\rightarrow \\infty} \\sum_{i: 0 \\leq i \\leq n\\delta + m} \\text{Pr}\\{S_n = i\\} $\n",
    "\n",
    "**(c)** $\\lim_{n\\rightarrow \\infty} \\sum_{i: n(\\delta - 1/m) \\leq i \\leq n(\\delta + 1/m)} \\text{Pr}\\{S_n = i\\} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  From Theorem 1.7.3, for $1/2 < \\alpha < 2/3$, there are constants $C$ and $n_0$ such that for all integers $k$ with $|k - n\\delta| \\leq n^\\alpha$,\n",
    "\n",
    "$$ \\text{Pr}\\{S_n = k\\} \\leq \\frac{1 + C n^{3\\alpha - 2}}{\\sqrt{2 \\pi n \\delta (1 - \\delta)}} \\exp \\left\\{ \\frac{-(k - n\\delta)^2}{2 n \\delta (1 - \\delta)} \\right\\} \\quad \\text{for } n \\geq n_0$$\n",
    "\n",
    "In particular, if $\\alpha = 1$, and for all integers $k$ such that $|k - n\\delta| \\leq m$, \n",
    "\n",
    "$$ \\text{Pr}\\{S_n = k\\} \\leq \\frac{1 + C n}{\\sqrt{2 \\pi n \\delta (1 - \\delta)}} \\exp \\left\\{ \\frac{-(k - n\\delta)^2}{2 n \\delta (1 - \\delta)} \\right\\} \\quad \\text{for } n \\geq n_0$$\n",
    "\n",
    "Adding up the inequalities,\n",
    "\n",
    "$$ \\sum_{i: n\\delta - m \\leq k \\leq n\\delta + m} \\text{Pr}\\{S_n = k\\} \\leq \\sum_{i: n\\delta - m \\leq k \\leq n\\delta + m} \\frac{1 + C n}{\\sqrt{2 \\pi n \\delta (1 - \\delta)}} \\exp \\left\\{ \\frac{-(k - n\\delta)^2}{2 n \\delta (1 - \\delta)} \\right\\}\n",
    "= (2m + 1) O(\\sqrt{n})O(e^{-n}) $$\n",
    "\n",
    "which has limit 0 as $n \\rightarrow \\infty$, so\n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty} \\sum_{i: n\\delta - m \\leq i \\leq n\\delta + m} \\text{Pr}\\{S_n = i\\} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** The central limit theorem states that\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr}\\left\\{ \\frac{S_n}{n} - \\overline{X} \\leq \\frac{\\sigma z}{\\sqrt{n}} \\right\\} = \\Phi(z) $$\n",
    "\n",
    "In particular, for $z = 0$, given that $\\overline{X} = \\text{E}[X_i] = \\delta$,\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr}\\left\\{ S_n \\leq n\\delta \\right\\} = \\frac{1}{2} $$\n",
    "\n",
    "Expanding that over all integer values of $S_n$,\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\sum_{i: 0\\leq i \\leq n\\delta }\\text{Pr}\\left\\{ S_n = i \\right\\} = \\frac{1}{2} $$\n",
    "\n",
    "and adding the terms from $n\\delta$ to $n\\delta + m$ from the result in (a), whose probability sum must be 0,\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\sum_{i: 0\\leq i \\leq n\\delta + m }\\text{Pr}\\left\\{ S_n = i \\right\\} = \\frac{1}{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  We have $\\text{E}[S_n] = \\sum_{i=1}^n \\text{E}[X_i] = n\\delta$, and by the weak law of large numbers, for any $\\epsilon > 0$,\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr} \\left\\{ \\left| \\frac{S_n}{n} - \\frac{\\text{E}[S_n]}{n} \\right| > \\epsilon \\right\\} = 0 $$\n",
    "\n",
    "so\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr} \\left\\{ \\left| \\frac{S_n}{n} - \\delta \\; \\right| > \\epsilon \\right\\} = 0 $$\n",
    "\n",
    "or, for the complementary event,\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr} \\left\\{ n( \\delta - \\epsilon) \\leq S_n \\leq n(\\delta + \\epsilon) \\right\\} = 1 $$\n",
    "\n",
    "In particular, for $\\epsilon = 1/m$, \n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr} \\left\\{ n \\left( \\delta - \\frac{1}{m}\\right) \\leq S_n \\leq n\\left(\\delta + \\frac{1}{m}\\right) \\right\\} = 1 $$\n",
    "\n",
    "Since $S_n$ can only assume integer values,\n",
    "\n",
    "$$ \\lim_{n\\rightarrow \\infty} \\sum_{i: n(\\delta - 1/m) \\leq i \\leq n(\\delta + 1/m)} \\text{Pr}\\{S_n = i\\} = 1$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
