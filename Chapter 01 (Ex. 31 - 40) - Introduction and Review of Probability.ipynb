{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.31 (Alternative approach 2 to the Markov inequality)**.\n",
    "\n",
    "**(a)**  Minimize $\\text{E}[Y]$ over all non-negative random variables such that $\\text{Pr}\\{Y \\geq b\\} \\geq \\beta$ for some given $b > 0$ and $0 < \\beta < 1$.  Hint:  Use a graphical argument similar to that in Figure 1.7.  What is the random variable that achieves this minimum.  Hint:  It is binary.\n",
    "\n",
    "**(b)**  Use (a) to prove the Markov inequality and also point out the distribution that meets the inequality with equality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The graphical argument is that $\\text{Pr}\\{Y \\geq b \\} = F^c_Y(b)$, the complement of the CDF computed at point $b$.  The inequality requires that this curve assume the value of at least $\\beta$ when evaluated at point $b$, and since $F_Y^c$ is non-increasing, $F_Y^c(y) \\geq \\beta$ for $y \\leq b$.  But $\\text{E}[Y] = \\int F_Y^c(y) dy$, so \n",
    "\n",
    "$$ \\text{E}[Y] \\int_0^\\infty F_Y^c(y) dy \\geq \\int_0^b \\beta dy = y \\beta $$\n",
    "\n",
    "In particular, $Y \\sim b \\cdot \\text{Binary}(\\beta)$ achieves this minimum, as $\\text{Pr}\\{Y \\geq b \\} = \\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** By choosing an arbitrary $b$ and then picking $\\beta = \\text{Pr}\\{Y \\geq b \\}$, from (a) we get\n",
    "\n",
    "$$ \\text{E}[Y] \\geq b \\cdot \\text{Pr}\\{Y \\geq b\\} \\Longrightarrow \\text{Pr}\\{Y \\geq b\\} \\leq \\frac{\\text{E}[Y]}{b} $$\n",
    "\n",
    "Once again, for the distribution $Y \\sim b \\cdot \\text{Binary}(\\beta)$ the equality case holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.32 (The one-sided Chebyshev inequality)**.  This inequality states that if a zero-mean random variable $X$ has variance $\\sigma^2$, then it satisfies the inequality\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2} \\quad \\text{for every } b > 0 \\tag{1.101} $$\n",
    "\n",
    "with equality for some $b$ only if $X$ is binary and $\\text{Pr}\\{X = b\\} = \\sigma^2 / (\\sigma^2 + b^2)$.  We prove this here using the same approach as in Exercise 1.31.  Let $X$ be a zero-mean random variable that satisfies $\\text{Pr}\\{X \\geq b\\} = \\beta$ for some $b > 0$ and $0 < \\beta < 1$.  The variance $\\sigma^2$ of $X$ can be expressed as\n",
    "\n",
    "$$ \\sigma^2 = \\int_{-\\infty}^{b^-} x^2 f_X(x) dx + \\int_b^{\\infty} x^2 f_X(x) dx \\tag{1.102}$$\n",
    "\n",
    "We will first minimize $\\sigma^2$ over all zero-mean $X$ satisfying $\\text{Pr}\\{X \\geq b\\} = \\beta$.\n",
    "\n",
    "**(a)**  Show that the second integral in (1.102) satisfies $\\int_b^\\infty x^2 f_X(x) dx \\geq b^2 \\beta$.\n",
    "\n",
    "**(b)**  Show that the first integral in (1.102) is constrained by \n",
    "\n",
    "$$ \n",
    "\\int_{-\\infty}^{b^-} f_X(x) dx = 1 - \\beta \n",
    "\\quad \\text{and} \\quad \n",
    "\\int_{-\\infty}^{b^-} x f_X(x) dx \\leq -b \\beta\n",
    "$$\n",
    "\n",
    "**(c)**  Minimize the first integral in (1.102) subject to the constraints in (b).  Hint: If you scale $f_X(x)$ up by $1 / (1 - \\beta)$, it integrates to 1 over $(-\\infty, b)$ and the second constraint becomes an expectation.  You can then minimize the first integral in (1.102) by inspection.\n",
    "\n",
    "**(d)**  Combine the results in (a) and (c) to show that $\\sigma^2 \\geq b^2 \\beta / (1 - \\beta)$.  Find the minimizing distribution.  Hint:  It is binary.\n",
    "\n",
    "**(e)**  Use (d) to establish (1.101).  Also show (trivially) that if $Y$ has a mean $\\overline{Y}$ and variance $\\sigma^2$, then $\\text{Pr}\\{Y - \\overline{Y} \\geq b\\} \\leq \\sigma^2 / (\\sigma^2 + b^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "$$ \\int_b^\\infty x^2 f_X(x) dx \\geq \\int_b^\\infty b^2 f_X(x) dx = b^2 \\left( \\int_b^\\infty f_X(x) dx \\right) = b^2 \\text{Pr}\\{ X \\geq b \\} = b^2 \\beta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "We have:\n",
    "\n",
    "$$ \\int_{-\\infty}^{b^-} f_X(x) dx = \\text{Pr}\\{X < b\\} = 1 - \\text{Pr}\\{ X \\geq b \\} = 1 - \\beta $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\int_{-\\infty}^{b^-} x f_X(x) dx &= \\text{E}[X] - \\int_b^\\infty x f_X(x) dx = 0 - \\int_b^\\infty x f_X(x) dx \\\\\n",
    "&\\leq - \\int_b^\\infty b f_X(x) dx = -b \\int_b^\\infty f_X(x) dx \\\\\n",
    "&= -b \\; \\text{Pr}\\{ X \\geq b \\} = -b \\beta\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Multiplying the first constraint in (b) by $1 / (1 - \\beta)$, we get\n",
    "\n",
    "$$ \\int_{-\\infty}^{b^-} \\frac{f_X(x)}{1 - \\beta} dx = \\frac{1 - \\beta}{1 - \\beta} = 1 $$\n",
    "\n",
    "This suggests we can define another probability density,\n",
    "\n",
    "$$ f_Y(y) = \\begin{cases}\n",
    "\\frac{f_X(y)}{1 - \\beta} &\\text{if } y < b \\\\\n",
    "0 &\\text{otherwise }\n",
    "\\end{cases} $$\n",
    "\n",
    "for which we can define the expectation \n",
    "\n",
    "$$ \\text{E}_Y[X] = \\int_{-\\infty}^\\infty x f_Y(x) dx = \\frac{1}{1 - \\beta} \\int_{-\\infty}^{b^-} x f_X(x) dx$$\n",
    "\n",
    "and so the second constraint can be expressed as\n",
    "\n",
    "$$ \\int_{-\\infty}^{b^-} x f_X(x) dx = (1 - \\beta) \\int_{-\\infty}^\\infty x f_Y(x) dx = (1 - \\beta)  \\text{E}_Y[X] \\leq -b \\beta$$\n",
    "\n",
    "or, with a bound on the expectation of $X$ under $Y$,\n",
    "\n",
    "$$  \\text{E}_Y[X] \\leq -b \\frac{\\beta}{1 - \\beta} $$\n",
    "\n",
    "Therefore, the expectation of $X^2$ under $Y$ is\n",
    "\n",
    "$$ \\text{E}_Y[X^2] = \\text{Var}_Y[X] + \\text{E}_Y[X]^2 \\geq \\text{E}_Y[X]^2 \\geq b^2 \\frac{\\beta^2}{(1 - \\beta)^2} $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\int_{-\\infty}^{b^-} x^2 f_X(x) dx = (1 - \\beta) \\text{E}_Y[X^2] \\geq b^2 \\frac{\\beta^2}{1 - \\beta} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  We have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma^2 &= \\int_{-\\infty}^{b^-} x^2 f_X(x) dx + \\int_b^{\\infty} x^2 f_X(x) dx \\\\\n",
    "&\\geq b^2 \\frac{\\beta^2}{1 - \\beta} + b^2 \\beta \\\\\n",
    "&= b^2 \\frac{\\beta}{1 - \\beta}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To find an example where the equality holds, consider a Bernoulli (binary) variable with parameter $p$, shifted to have mean 0:\n",
    "\n",
    "$$ X \\sim \\text{Bernoulli}(p) - p $$\n",
    "\n",
    "By construction, $\\text{E}[X] = 0$, and $\\sigma^2 = \\text{Var}[X] = p(1 - p)$.  Then, taking $b = 1 - p$,\n",
    "\n",
    "$$ \\beta = \\text{Pr}\\{X \\geq 1 - p\\} = p \\quad \\text{and} \\quad b^2 \\frac{\\beta}{1 - \\beta} = (1-p)^2 \\frac{p}{1 - p} = p(1 - p) = \\text{Var}[X] $$ \n",
    "\n",
    "The one-sided Chebyshev inequality also provides an equality bound at this $b$:\n",
    "\n",
    "$$ \\frac{\\sigma^2}{\\sigma^2 + b^2} = \\frac{p(1-p)}{p(1-p) + (1-p)^2} = \\frac{p}{p + (1 - p)} = p $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**  From (d),\n",
    "\n",
    "$$ \\sigma^2 \\geq b^2 \\frac{\\beta}{1 - \\beta} $$\n",
    "\n",
    "and since $g(x) = \\frac{x}{x + b^2}$ is increasing in $x$, $x > 0$,\n",
    "\n",
    "$$ \\frac{\\sigma^2}{\\sigma^2 + b^2} \\geq \\frac{b^2\\frac{\\beta}{1 - \\beta}}{b^2\\frac{\\beta}{1 - \\beta} + b^2} = \\frac{\\beta}{\\beta + (1 - \\beta)} = \\beta = \\text{Pr}\\{X \\geq b \\}$$\n",
    "\n",
    "which is our result.\n",
    "\n",
    "Also, trivially, if $Y$ has mean $\\overline{Y}$, we can define the random variable $X = Y - \\overline{Y}$ with $\\text{Var}(X) = \\text{Var}(Y) = \\sigma^2$, so $\\text{Pr}\\{X \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2}$ implies $\\text{Pr}\\{Y - \\overline{Y} \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.33 (Proof of (1.48))**.  Here we show that if $X$ is a zero-mean random variable with a variance $\\sigma^2$, then the median $\\alpha$ satisfies $|\\alpha| \\leq \\sigma$.\n",
    "\n",
    "**(a)**  First show that $|\\alpha| \\leq \\sigma$ for the special case where $X$ is binary with equiprobable values at $\\pm \\sigma$.\n",
    "\n",
    "**(b)** For all zero-mean random variables $X$ with variance $\\sigma^2$ other than the special case in (a), show that\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\sigma\\} < 0.5 $$\n",
    "\n",
    "Hint:  Use the one-sided Chebyshev inequality of Exercise 1.32.\n",
    "\n",
    "**(c)** Show that $\\text{Pr}\\{X \\geq \\alpha\\} \\geq 0.5$.  Other than the special case in (a), show that this implies that $\\alpha < \\sigma$.\n",
    "\n",
    "**(d)** Other than the special case in (a), show that $|\\alpha| < \\sigma$.  Hint:  Repeat (b) and (c) for the random variable $-X$.  You have then shown that $|\\alpha| \\leq \\sigma$ with equality only for the binary case with values $\\pm \\sigma$.  For random variables $Y$ with a non-zero mean, this shows that $|\\alpha - \\overline{Y}| \\leq \\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  For this particular distribution, the interval of medians is $(-\\sigma, \\sigma)$, since for any $\\alpha$ in this interval $\\text{Pr}\\{ X \\leq \\alpha \\} = \\text{Pr}\\{ X \\geq \\alpha \\} = 0.5$ (counting only the probability mass at $-\\sigma$).  Then $\\alpha \\in (-\\sigma, \\sigma)$ implies $|\\alpha| \\leq \\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Applying the one-sided Chebyshev inequality from Exercise 1.32,\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2} \\quad \\text{for every } b > 0 $$\n",
    "\n",
    "Choose $b = \\sigma$, and we get\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\sigma \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + \\sigma^2} = 0.5 $$\n",
    "\n",
    "We can also demonstrate the one-sided Chebyshev inequality directly from Markov's inequality; for any $u \\geq 0$,\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq b\\} \\leq \\text{Pr}\\{(X + u)^2 \\geq (b + u)^2\\} \\leq \\frac{\\text{E}[(X + u)^2]}{(b + u)^2} = \\frac{\\sigma^2 + u^2}{(b + u)^2}$$\n",
    "\n",
    "and the bound is minimized by $u = \\sigma^2 / b$, leading to \n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2} $$\n",
    "\n",
    "Therefore, equality in the one-sided Chebyshev occurs if and only if we have equality in the Markov distribution above, which implies the distribution given in (a).  So, for all other cases,\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\sigma \\} < 0.5 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  By definition, any median $\\alpha$ satisfies $\\text{Pr}\\{X \\geq \\alpha\\} \\geq 0.5$, and so\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\alpha\\} \\geq 0.5 \\geq \\text{Pr}\\{X \\geq \\sigma \\} $$\n",
    "\n",
    "For distributions other than the distribution in (a), the second inequality is not an equality, and so\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\alpha\\} > \\text{Pr}\\{X \\geq \\sigma \\} $$\n",
    "\n",
    "Since the complementary CDF $F_X^c(x) = \\text{Pr}\\{ X \\geq x \\}$ is non-increasing, this implies that $\\alpha < \\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Repeating the argument of (b) and (c) for the random variable $-X$ following a distribution other than the one in (a), its medians $\\alpha'$ satisfy $\\alpha' < \\sigma$. But if $\\alpha'$ is a median of $-X$, then $-\\alpha'$ is a median of $X$, so this implies that $\\alpha > -\\sigma$.  Therefore, we have $-\\sigma < \\alpha < \\sigma$, or $|\\alpha| < \\sigma$.  (For the distribution in (a), the same argument applies with equalities instead.)\n",
    "\n",
    "It does follow that we can translate any arbitrary variable $Y$ to a zero-mean variable $X$ doing $X = Y - \\overline{Y}$; the medians also get translated by $-\\overline{Y}$, and thus we have shown that $|\\alpha - \\overline{Y}| \\leq \\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.34**.  We stressed the importance of the mean of a random variable $X$ in terms of association with its sample average via the WLLN.  Here we show that in essence the WLLN allows us to evaluate the entire CDF, say $F_X(x)$ of $X$, via sufficiently many independent sample values of $X$.\n",
    "\n",
    "**(a)** For any given $y$, let $\\mathbb{I}_j(y)$ be the indicator function of the event $\\{X_j \\leq y\\}$, where $X_1, X_2, \\dots, X_j, \\dots$ are IID random variables with the CDF $F_X(x)$.  State the WLLN for the IID random variables $\\{ \\mathbb{I}_1(y), \\mathbb{I}_2(y), \\dots\\}$.\n",
    "\n",
    "**(b)** Does the answer to (a) require $X$ to have a mean or variance?\n",
    "\n",
    "**(c)** Suggest a procedure for evaluating the median of $X$ from the sample values of $X_1, X_2, \\dots$.  Assume that $X$ is a continuous random variable and that its PDF is positive in an open interval around the median.  You need not be precise, but try to think the issue through carefully.\n",
    "\n",
    "What you have seen here, without stating it precisely or proving it is that the median has a law or large numbers associated with it, saying that the sample median of $n$ IID samples of a random variable is close to the true median with high probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  Consider the sample averages of the indicator sequence,\n",
    "\n",
    "$$ S_{\\mathbb{I}_n(y)} = \\frac{1}{n} \\sum_{j=1}^n \\mathbb{I}_j(y) $$\n",
    "\n",
    "Its expected value is \n",
    "\n",
    "$$ \\text{E}[S_{\\mathbb{I}_n(y)}] = \\text{E}[\\mathbb{I}_1(y)] = \\text{Pr}\\{X_1 \\leq y\\} = F_X(y) $$\n",
    "\n",
    "and so the WLLN states that\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr} \\left\\{ \\left| \\frac{S_{\\mathbb{I}_n(y)} }{n} - F_X(y) \\right| > \\epsilon \\right\\} = 0 \\quad \\text{for every } \\epsilon > 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  No.  The indicator variable sequence is required to have a mean, but that follows from the sequence approaching the CDF of $X$ at a given point $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  For any given $y$, we can approximate $F_X(y)$ by computing the sample average of the sequence $\\{\\mathbb{I}_1(y), \\mathbb{I}_2(y), \\dots \\}$.  For the (unknown) median $\\tilde{\\mu}$, we have $F_X(\\tilde{\\mu}) = 1/2$, so half of the terms of the original sequence $X_1, X_2, \\dots$, would be under $\\tilde{\\mu}$ and half over $\\tilde{\\mu}$:\n",
    "\n",
    "$$ \\text{Pr}\\{ X_i \\leq \\tilde{\\mu} \\} = \\text{Pr}\\{ X_i \\geq \\tilde{\\mu} \\} = 1/2 $$\n",
    "\n",
    "Therefore, if we get the sample median of the sequence of $X_i$'s $\\overline{\\mu}_n$, the weak law of large numbers state that\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr} \\left\\{ \\left| \\frac{S_{\\mathbb{I}_n(\\overline{\\mu}_n)} }{n} - F_X(\\overline{\\mu}_n) \\right| > \\epsilon \\right\\} = 0 $$\n",
    "\n",
    "But the sample average of the indicator sequence for the sample median converges to 1/2 by construction -- we defined $\\overline{\\mu}_n$ as the median of $\\{X_1, \\dots, X_n\\}$ -- so\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\text{Pr} \\left\\{ \\left| \\frac{S_{\\mathbb{I}_n(\\overline{\\mu}_n)} }{n} - \\frac{1}{2} \\right| > \\epsilon \\right\\} = 0 $$\n",
    " \n",
    "which implies $ \\lim_{n \\rightarrow \\infty} F_X(\\overline{\\mu}_n) = 1/2$ and so $ \\lim_{n \\rightarrow \\infty} \\overline{\\mu}_n = \\tilde{\\mu} $, that is, the sample median converges to the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.35**. \n",
    "\n",
    "**(a)** Show that for any $0 < k < n$\n",
    "\n",
    "$$ \\binom{n}{k+1} \\leq \\binom{n}{k} \\frac{n - k}{k} $$\n",
    "\n",
    "**(b)** Extend (a) to show that, for all $\\ell \\leq n - k$,\n",
    "\n",
    "$$ \\binom{n}{k + \\ell} \\leq \\binom{n}{k} \\left[ \\frac{n - k}{k} \\right]^\\ell $$\n",
    "\n",
    "**(c)** Let $\\tilde{p} = k / n$ and $\\tilde{q} = 1 - \\tilde{p}$.  Let $S_n$ be the sum of $n$ binary IID random variables with $p_X(0) = q$ and $p_X(1) = p$.  Show that for all $\\ell \\leq n - k$,\n",
    "\n",
    "$$ p_{S_n}(k + \\ell) \\leq p_{S_n}(k) \\left( \\frac{\\tilde{q} p}{\\tilde{p} q} \\right)^\\ell $$\n",
    "\n",
    "**(d)** For $k / n > p$, show that\n",
    "\n",
    "$$ \\text{Pr}\\{ S_n \\geq pn \\} \\leq \\frac{\\tilde{p} q}{\\tilde{p} - p} p_{S_n}(k) $$\n",
    "\n",
    "**(e)** Now let $\\ell$ be fixed and $k = \\lceil n \\tilde{p} \\rceil$ for fixed $\\tilde{p}$ such that $1 > \\tilde{p} > p$.  Argue that as $n \\rightarrow \\infty$,\n",
    "\n",
    "$$\n",
    "p_{S_n}(k + \\ell) \\sim p_{S_n}(k) \\left( \\frac{\\tilde{q} p}{\\tilde{p} q} \\right)^\\ell\n",
    "\\quad \\text{and} \\quad\n",
    "\\text{Pr}\\{S_n \\geq pn \\} \\sim \\frac{\\tilde{p} q}{\\tilde{p} - p} p_{S_n}(k)\n",
    "$$\n",
    "\n",
    "where $a(n) \\sim b(n)$ means that $\\lim_{n \\rightarrow \\infty} a(n) / b(n) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "$$ \\binom{n}{k+1} = \\frac{n!}{(k + 1)! (n - k - 1)!} = \\frac{n!}{k!(n - k)!} \\frac{n - k}{k+1} = \\binom{n}{k} \\frac{n - k}{k+1} < \\binom{n}{k} \\frac{n - k}{k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  For $\\ell \\geq 1$, from (a),\n",
    "\n",
    "$$ \\binom{n}{k + \\ell} \\leq \\binom{n}{k + \\ell - 1} \\frac{n - k}{k + \\ell - 1} \\leq \\binom{n}{k + \\ell - 1} \\frac{n - k}{k} $$\n",
    "\n",
    "Applying this result $\\ell$ times we get\n",
    "\n",
    "$$ \\binom{n}{k + \\ell} \\leq \\binom{n}{k + \\ell - 1} \\frac{n - k}{k} \\leq \\binom{n}{k + \\ell - 2} \\left[\\frac{n - k}{k}\\right]^2 \\leq \\cdots \\leq \\binom{n}{k} \\left[\\frac{n - k}{k}\\right]^\\ell $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Using the fact that the sum of $n$ IID variables is binomial,\n",
    "\n",
    "$$ p_{S_n}(k + \\ell) = \\binom{n}{k + \\ell} p^{k + \\ell} q^{n - k - \\ell} \n",
    "\\quad \\text{and} \\quad\n",
    "p_{S_n}(k) = \\binom{n}{k} p^k q^{n - k} \n",
    "$$\n",
    "\n",
    "Replacing it into the statement we want to prove,\n",
    "\n",
    "$$ \n",
    "\\binom{n}{k + \\ell} p^{k + \\ell} q^{n - k - \\ell} \\leq \\binom{n}{k} p^k q^{n - k} \\left( \\frac{\\tilde{q} p}{\\tilde{p} q} \\right)^\\ell \n",
    "$$\n",
    "\n",
    "or, isolating $\\binom{n}{k + \\ell}$,\n",
    "\n",
    "$$\n",
    "\\binom{n}{k + \\ell} \\leq \\binom{n}{k} \\left( \\frac{\\tilde{q}}{\\tilde{p}} \\right)^\\ell = \\binom{n}{k} \\left[ \\frac{n - k}{k}\\right]^\\ell\n",
    "$$\n",
    "\n",
    "which is the result from (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  This result follows by adding the inequalities in (c) for $\\ell = 0, \\dots, n - k$ -- the left hand side is the complementary CDF of the binomial distribution, obtained by adding all the probability mass for possible values of $S_n$, while the right hand side is a geometric sum times a probability mass function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**  These bounds are tight, as all inequalities are built from the inequalities in (b), which replace $c(k) = (k +1)(k +2) \\cdots (k + \\ell) $ with $d(k) = k^\\ell$.  For a fixed $\\ell$, $c(n) \\sim d(n)$, and the same applies to all inequalities deduced from it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
