{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.31 (Alternative approach 2 to the Markov inequality)**.\n",
    "\n",
    "**(a)**  Minimize $\\text{E}[Y]$ over all non-negative random variables such that $\\text{Pr}\\{Y \\geq b\\} \\geq \\beta$ for some given $b > 0$ and $0 < \\beta < 1$.  Hint:  Use a graphical argument similar to that in Figure 1.7.  What is the random variable that achieves this minimum.  Hint:  It is binary.\n",
    "\n",
    "**(b)**  Use (a) to prove the Markov inequality and also point out the distribution that meets the inequality with equality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The graphical argument is that $\\text{Pr}\\{Y \\geq b \\} = F^c_Y(b)$, the complement of the CDF computed at point $b$.  The inequality requires that this curve assume the value of at least $\\beta$ when evaluated at point $b$, and since $F_Y^c$ is non-increasing, $F_Y^c(y) \\geq \\beta$ for $y \\leq b$.  But $\\text{E}[Y] = \\int F_Y^c(y) dy$, so \n",
    "\n",
    "$$ \\text{E}[Y] \\int_0^\\infty F_Y^c(y) dy \\geq \\int_0^b \\beta dy = y \\beta $$\n",
    "\n",
    "In particular, $Y \\sim b \\cdot \\text{Binary}(\\beta)$ achieves this minimum, as $\\text{Pr}\\{Y \\geq b \\} = \\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** By choosing an arbitrary $b$ and then picking $\\beta = \\text{Pr}\\{Y \\geq b \\}$, from (a) we get\n",
    "\n",
    "$$ \\text{E}[Y] \\geq b \\cdot \\text{Pr}\\{Y \\geq b\\} \\Longrightarrow \\text{Pr}\\{Y \\geq b\\} \\leq \\frac{\\text{E}[Y]}{b} $$\n",
    "\n",
    "Once again, for the distribution $Y \\sim b \\cdot \\text{Binary}(\\beta)$ the equality case holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.32 (The one-sided Chebyshev inequality)**.  This inequality states that if a zero-mean random variable $X$ has variance $\\sigma^2$, then it satisfies the inequality\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2} \\quad \\text{for every } b > 0 \\tag{1.101} $$\n",
    "\n",
    "with equality for some $b$ only if $X$ is binary and $\\text{Pr}\\{X = b\\} = \\sigma^2 / (\\sigma^2 + b^2)$.  We prove this here using the same approach as in Exercise 1.31.  Let $X$ be a zero-mean random variable that satisfies $\\text{Pr}\\{X \\geq b\\} = \\beta$ for some $b > 0$ and $0 < \\beta < 1$.  The variance $\\sigma^2$ of $X$ can be expressed as\n",
    "\n",
    "$$ \\sigma^2 = \\int_{-\\infty}^{b^-} x^2 f_X(x) dx + \\int_b^{\\infty} x^2 f_X(x) dx \\tag{1.102}$$\n",
    "\n",
    "We will first minimize $\\sigma^2$ over all zero-mean $X$ satisfying $\\text{Pr}\\{X \\geq b\\} = \\beta$.\n",
    "\n",
    "**(a)**  Show that the second integral in (1.102) satisfies $\\int_b^\\infty x^2 f_X(x) dx \\geq b^2 \\beta$.\n",
    "\n",
    "**(b)**  Show that the first integral in (1.102) is constrained by \n",
    "\n",
    "$$ \n",
    "\\int_{-\\infty}^{b^-} f_X(x) dx = 1 - \\beta \n",
    "\\quad \\text{and} \\quad \n",
    "\\int_{-\\infty}^{b^-} x f_X(x) dx \\leq -b \\beta\n",
    "$$\n",
    "\n",
    "**(c)**  Minimize the first integral in (1.102) subject to the constraints in (b).  Hint: If you scale $f_X(x)$ up by $1 / (1 - \\beta)$, it integrates to 1 over $(-\\infty, b)$ and the second constraint becomes an expectation.  You can then minimize the first integral in (1.102) by inspection.\n",
    "\n",
    "**(d)**  Combine the results in (a) and (c) to show that $\\sigma^2 \\geq b^2 \\beta / (1 - \\beta)$.  Find the minimizing distribution.  Hint:  It is binary.\n",
    "\n",
    "**(e)**  Use (d) to establish (1.101).  Also show (trivially) that if $Y$ has a mean $\\overline{Y}$ and variance $\\sigma^2$, then $\\text{Pr}\\{Y - \\overline{Y} \\geq b\\} \\leq \\sigma^2 / (\\sigma^2 + b^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "$$ \\int_b^\\infty x^2 f_X(x) dx \\geq \\int_b^\\infty b^2 f_X(x) dx = b^2 \\left( \\int_b^\\infty f_X(x) dx \\right) = b^2 \\text{Pr}\\{ X \\geq b \\} = b^2 \\beta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "We have:\n",
    "\n",
    "$$ \\int_{-\\infty}^{b^-} f_X(x) dx = \\text{Pr}\\{X < b\\} = 1 - \\text{Pr}\\{ X \\geq b \\} = 1 - \\beta $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\int_{-\\infty}^{b^-} x f_X(x) dx &= \\text{E}[X] - \\int_b^\\infty x f_X(x) dx = 0 - \\int_b^\\infty x f_X(x) dx \\\\\n",
    "&\\leq - \\int_b^\\infty b f_X(x) dx = -b \\int_b^\\infty f_X(x) dx \\\\\n",
    "&= -b \\; \\text{Pr}\\{ X \\geq b \\} = -b \\beta\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Multiplying the first constraint in (b) by $1 / (1 - \\beta)$, we get\n",
    "\n",
    "$$ \\int_{-\\infty}^{b^-} \\frac{f_X(x)}{1 - \\beta} dx = \\frac{1 - \\beta}{1 - \\beta} = 1 $$\n",
    "\n",
    "This suggests we can define another probability density,\n",
    "\n",
    "$$ f_Y(y) = \\begin{cases}\n",
    "\\frac{f_X(y)}{1 - \\beta} &\\text{if } y < b \\\\\n",
    "0 &\\text{otherwise }\n",
    "\\end{cases} $$\n",
    "\n",
    "for which we can define the expectation \n",
    "\n",
    "$$ \\text{E}_Y[X] = \\int_{-\\infty}^\\infty x f_Y(x) dx = \\frac{1}{1 - \\beta} \\int_{-\\infty}^{b^-} x f_X(x) dx$$\n",
    "\n",
    "and so the second constraint can be expressed as\n",
    "\n",
    "$$ \\int_{-\\infty}^{b^-} x f_X(x) dx = (1 - \\beta) \\int_{-\\infty}^\\infty x f_Y(x) dx = (1 - \\beta)  \\text{E}_Y[X] \\leq -b \\beta$$\n",
    "\n",
    "or, with a bound on the expectation of $X$ under $Y$,\n",
    "\n",
    "$$  \\text{E}_Y[X] \\leq -b \\frac{\\beta}{1 - \\beta} $$\n",
    "\n",
    "Therefore, the expectation of $X^2$ under $Y$ is\n",
    "\n",
    "$$ \\text{E}_Y[X^2] = \\text{Var}_Y[X] + \\text{E}_Y[X]^2 \\geq \\text{E}_Y[X]^2 \\geq b^2 \\frac{\\beta^2}{(1 - \\beta)^2} $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\int_{-\\infty}^{b^-} x^2 f_X(x) dx = (1 - \\beta) \\text{E}_Y[X^2] \\geq b^2 \\frac{\\beta^2}{1 - \\beta} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  We have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma^2 &= \\int_{-\\infty}^{b^-} x^2 f_X(x) dx + \\int_b^{\\infty} x^2 f_X(x) dx \\\\\n",
    "&\\geq b^2 \\frac{\\beta^2}{1 - \\beta} + b^2 \\beta \\\\\n",
    "&= b^2 \\frac{\\beta}{1 - \\beta}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To find an example where the equality holds, consider a Bernoulli (binary) variable with parameter $p$, shifted to have mean 0:\n",
    "\n",
    "$$ X \\sim \\text{Bernoulli}(p) - p $$\n",
    "\n",
    "By construction, $\\text{E}[X] = 0$, and $\\sigma^2 = \\text{Var}[X] = p(1 - p)$.  Then, taking $b = 1 - p$,\n",
    "\n",
    "$$ \\beta = \\text{Pr}\\{X \\geq 1 - p\\} = p \\quad \\text{and} \\quad b^2 \\frac{\\beta}{1 - \\beta} = (1-p)^2 \\frac{p}{1 - p} = p(1 - p) = \\text{Var}[X] $$ \n",
    "\n",
    "The one-sided Chebyshev inequality also provides an equality bound at this $b$:\n",
    "\n",
    "$$ \\frac{\\sigma^2}{\\sigma^2 + b^2} = \\frac{p(1-p)}{p(1-p) + (1-p)^2} = \\frac{p}{p + (1 - p)} = p $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**  From (d),\n",
    "\n",
    "$$ \\sigma^2 \\geq b^2 \\frac{\\beta}{1 - \\beta} $$\n",
    "\n",
    "and since $g(x) = \\frac{x}{x + b^2}$ is increasing in $x$, $x > 0$,\n",
    "\n",
    "$$ \\frac{\\sigma^2}{\\sigma^2 + b^2} \\geq \\frac{b^2\\frac{\\beta}{1 - \\beta}}{b^2\\frac{\\beta}{1 - \\beta} + b^2} = \\frac{\\beta}{\\beta + (1 - \\beta)} = \\beta = \\text{Pr}\\{X \\geq b \\}$$\n",
    "\n",
    "which is our result.\n",
    "\n",
    "Also, trivially, if $Y$ has mean $\\overline{Y}$, we can define the random variable $X = Y - \\overline{Y}$ with $\\text{Var}(X) = \\text{Var}(Y) = \\sigma^2$, so $\\text{Pr}\\{X \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2}$ implies $\\text{Pr}\\{Y - \\overline{Y} \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.33 (Proof of (1.48))**.  Here we show that if $X$ is a zero-mean random variable with a variance $\\sigma^2$, then the median $\\alpha$ satisfies $|\\alpha| \\leq \\sigma$.\n",
    "\n",
    "**(a)**  First show that $|\\alpha| \\leq \\sigma$ for the special case where $X$ is binary with equiprobable values at $\\pm \\sigma$.\n",
    "\n",
    "**(b)** For all zero-mean random variables $X$ with variance $\\sigma^2$ other than the special case in (a), show that\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\sigma\\} < 0.5 $$\n",
    "\n",
    "Hint:  Use the one-sided Chebyshev inequality of Exercise 1.32.\n",
    "\n",
    "**(c)** Show that $\\text{Pr}\\{X \\geq \\alpha\\} \\geq 0.5$.  Other than the special case in (a), show that this implies that $\\alpha < \\sigma$.\n",
    "\n",
    "**(d)** Other than the special case in (a), show that $|\\alpha| < \\sigma$.  Hint:  Repeat (b) and (c) for the random variable $-X$.  You have then shown that $|\\alpha| \\leq \\sigma$ with equality only for the binary case with values $\\pm \\sigma$.  For random variables $Y$ with a non-zero mean, this shows that $|\\alpha - \\overline{Y}| \\leq \\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  For this particular distribution, the interval of medians is $(-\\sigma, \\sigma)$, since for any $\\alpha$ in this interval $\\text{Pr}\\{ X \\leq \\alpha \\} = \\text{Pr}\\{ X \\geq \\alpha \\} = 0.5$ (counting only the probability mass at $-\\sigma$).  Then $\\alpha \\in (-\\sigma, \\sigma)$ implies $|\\alpha| \\leq \\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Applying the one-sided Chebyshev inequality from Exercise 1.32,\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2} \\quad \\text{for every } b > 0 $$\n",
    "\n",
    "Choose $b = \\sigma$, and we get\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\sigma \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + \\sigma^2} = 0.5 $$\n",
    "\n",
    "We can also demonstrate the one-sided Chebyshev inequality directly from Markov's inequality; for any $u \\geq 0$,\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq b\\} \\leq \\text{Pr}\\{(X + u)^2 \\geq (b + u)^2\\} \\leq \\frac{\\text{E}[(X + u)^2]}{(b + u)^2} = \\frac{\\sigma^2 + u^2}{(b + u)^2}$$\n",
    "\n",
    "and the bound is minimized by $u = \\sigma^2 / b$, leading to \n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq b \\} \\leq \\frac{\\sigma^2}{\\sigma^2 + b^2} $$\n",
    "\n",
    "Therefore, equality in the one-sided Chebyshev occurs if and only if we have equality in the Markov distribution above, which implies the distribution given in (a).  So, for all other cases,\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\sigma \\} < 0.5 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  By definition, any median $\\alpha$ satisfies $\\text{Pr}\\{X \\geq \\alpha\\} \\geq 0.5$, and so\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\alpha\\} \\geq 0.5 \\geq \\text{Pr}\\{X \\geq \\sigma \\} $$\n",
    "\n",
    "For distributions other than the distribution in (a), the second inequality is not an equality, and so\n",
    "\n",
    "$$ \\text{Pr}\\{X \\geq \\alpha\\} > \\text{Pr}\\{X \\geq \\sigma \\} $$\n",
    "\n",
    "Since the complementary CDF $F_X^c(x) = \\text{Pr}\\{ X \\geq x \\}$ is non-increasing, this implies that $\\alpha < \\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Repeating the argument of (b) and (c) for the random variable $-X$ following a distribution other than the one in (a), its medians $\\alpha'$ satisfy $\\alpha' < \\sigma$. But if $\\alpha'$ is a median of $-X$, then $-\\alpha'$ is a median of $X$, so this implies that $\\alpha > -\\sigma$.  Therefore, we have $-\\sigma < \\alpha < \\sigma$, or $|\\alpha| < \\sigma$.  (For the distribution in (a), the same argument applies with equalities instead.)\n",
    "\n",
    "It does follow that we can translate any arbitrary variable $Y$ to a zero-mean variable $X$ doing $X = Y - \\overline{Y}$; the medians also get translated by $-\\overline{Y}$, and thus we have shown that $|\\alpha - \\overline{Y}| \\leq \\sigma$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
